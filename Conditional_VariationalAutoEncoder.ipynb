{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Built using guidance from https://arxiv.org/pdf/1512.09300.pdf\n",
    "\n",
    "Features:\n",
    "  * Uses ELU activations\n",
    "  * Deconvolution uses upscaling unpool layer before affine operator, rather than spacing with zeros\n",
    "  * Batch normalization after each transformation\n",
    "  * Dropout layer after activation\n",
    "  * abs-sum image loss rather than cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Best result so far is 10 epochs of the first training batch, with the prior and the prediction weighted equally.\n",
    "\n",
    "Running another 10 epochs doesn't hurt the similarity results, but it does make the reconstructions worse.\n",
    "\n",
    "Not quite as good after 10 epochs with regularization=0.1 (down-weighted prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running TensorFlow version 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "print(\"running TensorFlow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control memory usage\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report OOM details\n",
    "\n",
    "run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: read from tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_ROOT = '../../data/'\n",
    "\n",
    "RUN_NAME = 'cvae/vae_004'\n",
    "SUMMARY_DIR = os.path.join(LOG_ROOT, 'logs', RUN_NAME)\n",
    "MODEL_DIR = os.path.join(LOG_ROOT, 'models', RUN_NAME)\n",
    "MODEL_GRAPH = os.path.join(MODEL_DIR, 'vae.meta')\n",
    "MODEL_PREFIX = os.path.join(MODEL_DIR, 'vae')\n",
    "\n",
    "DATA_SIZE = 'all_packs'\n",
    "EXPT_NAME = 'expt_004'\n",
    "VIS_NAME = 'vis_001'\n",
    "DATA_ROOT = '/var/data/processed'\n",
    "\n",
    "EXPT_DIR = os.path.join(DATA_ROOT, EXPT_NAME, 'data')\n",
    "TFRECORDS_DIR = os.path.join('/var/data/original/tfrecords/', DATA_SIZE)\n",
    "EMBEDDINGS_DIR = os.path.join(EXPT_DIR, 'embeddings')\n",
    "ENCODED_IMAGES_DIR =  os.path.join(EXPT_DIR, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_256_pattern = re.compile('^train_(?P<block_id>[0-9]{3}).tfrecords')\n",
    "validate_256_pattern = re.compile('^validate_(?P<block_id>[0-9]{3}).tfrecords')\n",
    "test_256_pattern = re.compile('^test_(?P<block_id>[0-9]{3}).tfrecords')\n",
    "golden_256_pattern = re.compile('golden_(?P<block_id>[0-9]{3}).tfrecords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALL_TFRECORDS = os.listdir(TFRECORDS_DIR)\n",
    "def get_sorted_records(pattern, directory):\n",
    "     return [ \\\n",
    "         os.path.join(directory, _file) \\\n",
    "         for _file in \\\n",
    "         sorted([_m[0] for _m in \\\n",
    "             [pattern.match(_f) for _f in os.listdir(directory)] if _m]) \\\n",
    "     ]\n",
    "\n",
    "GOLDEN_TFRECORDS = get_sorted_records(golden_256_pattern, TFRECORDS_DIR)\n",
    "TRAIN_TFRECORDS = get_sorted_records(train_256_pattern, TFRECORDS_DIR)\n",
    "VALIDATE_TFRECORDS = get_sorted_records(validate_256_pattern, TFRECORDS_DIR)\n",
    "TEST_TFRECORDS = get_sorted_records(test_256_pattern, TFRECORDS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/var/data/original/tfrecords/all_packs/train_003.tfrecords'],\n",
       " ['/var/data/original/tfrecords/all_packs/validate_003.tfrecords'],\n",
       " ['/var/data/original/tfrecords/all_packs/golden_003.tfrecords'],\n",
       " ['/var/data/original/tfrecords/all_packs/test_003.tfrecords'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_TFRECORDS, VALIDATE_TFRECORDS,GOLDEN_TFRECORDS, TEST_TFRECORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: read from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_image(path, size):\n",
    "    ''' load grayscale image from file; resize if necessary\n",
    "    '''\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if img is None:\n",
    "        raise ValueError(\"empty image: {}\".format(path))\n",
    "\n",
    "    if size:\n",
    "        img = cv2.resize(img, (size,size), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR = '/var/data/original/data/images'\n",
    "TEST_JSON = '/var/data/original/labels/test_data.json'\n",
    "VALIDATE_JSON = '/var/data/original/labels/validate_data.json'\n",
    "TRAIN_JSON = '/var/data/original/labels/train_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "\n",
    "def gen_images(images_dir, json_dir):\n",
    "    \n",
    "    images = []\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    with open(json_dir, 'r') as fi:\n",
    "        records = json.load(fi)\n",
    "        \n",
    "    json_filenames = []\n",
    "    filenames2labels = defaultdict(list)\n",
    "\n",
    "\n",
    "    for r in records:\n",
    "        json_filenames.append(r['filename'])\n",
    "        filenames2labels[r['filename']].append(r['labels'])\n",
    "\n",
    "    for image_path in glob.glob(os.path.join(images_dir,'*')):\n",
    "        filename = image_path[-16:]\n",
    "        if(filename in json_filenames):\n",
    "            filenames.append(filename)\n",
    "            labels.append(filenames2labels[filename][0])\n",
    "            images.append(_load_image(image_path, 256))\n",
    "            \n",
    "    return filenames, images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames, train_images, train_labels = gen_images(IMAGES_DIR, TRAIN_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(SUMMARY_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode(serialized_example):\n",
    "    '''Parses an image and label from the given `serialized_example`\n",
    "    '''\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'filename': tf.FixedLenFeature([], tf.string),\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "            #'view': tf.FixedLenFeature([], tf.string),\n",
    "            #'gender': tf.FixedLenFeature([], tf.string),\n",
    "            #'age': tf.FixedLenFeature([], tf.int64),\n",
    "            'labels': tf.FixedLenSequenceFeature( [], dtype=tf.int64, default_value=-1,allow_missing=True)\n",
    "            })\n",
    "       \n",
    "\n",
    "    # Convert from a scalar string tensor\n",
    "    filename = tf.cast(features['filename'], tf.string)\n",
    "    image = tf.decode_raw(features['image'], tf.float32)\n",
    "    #view = tf.cast(features['view'], tf.string)\n",
    "    #gender = tf.cast(features['gender'], tf.string)\n",
    "    #age = tf.cast(features['age'], tf.int32)\n",
    "    labels = tf.cast(features['labels'], tf.int32)\n",
    "    \n",
    "    return filename, image, labels\n",
    "\n",
    "def _filter(filename, image, labels):\n",
    "    \n",
    "    sub_string = tf.substr(filename,3,1)\n",
    "    return tf.equal(sub_string, \"0\")\n",
    "\n",
    "def _augment(filename, image, labels):\n",
    "    '''Placeholder for data augmentation\n",
    "    '''\n",
    "    image = tf.reshape(image, [256, 256, 1])\n",
    "    return filename, image, labels\n",
    "\n",
    "\n",
    "def _normalize(filename, image, labels):\n",
    "    '''Convert `image` from [0, 255] -> [-0.5, 0.5] floats\n",
    "    '''\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "    return filename, image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs(filenames, batch_size, num_epochs, num_shards, shard_index):\n",
    "    ''' Reads input data num_epochs times or forever if num_epochs is None\n",
    "        returns dataset, iterator pair\n",
    "    '''\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        # TFRecordDataset opens a binary file and reads one record at a time.\n",
    "        # `filename` could also be a list of filenames, which will be read in order.\n",
    "       \n",
    "        \n",
    "        dataset = tf.data.TFRecordDataset(filenames)\n",
    "        \n",
    "        # The map transformation takes a function and applies it to every element\n",
    "        # of the dataset.\n",
    "        \n",
    "        dataset = dataset.map(_decode)\n",
    "        dataset = dataset.shard(num_shards, shard_index)\n",
    "        #dataset = dataset.filter(_filter)\n",
    "        dataset = dataset.map(_augment)\n",
    "        dataset = dataset.map(_normalize)\n",
    "\n",
    "        # The shuffle transformation uses a finite-sized buffer to shuffle elements\n",
    "        # in memory. The parameter is the number of elements in the buffer. For\n",
    "        # completely uniform shuffling, set the parameter to be the same as the\n",
    "        # number of elements in the dataset.\n",
    "        dataset = dataset.shuffle(1000 + 3 * batch_size)\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    return dataset, iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "DROPOUT = 0.7\n",
    "REGULARIZATION = 0.1\n",
    "\n",
    "DISPLAY_EVERY = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summary(x, name):\n",
    "    with tf.variable_scope(name):\n",
    "        mean = tf.reduce_mean(x)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(x))\n",
    "        tf.summary.scalar('min', tf.reduce_min(x))\n",
    "        tf.summary.histogram('histogram', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpool operation doesn't yet exist in TF\n",
    "\n",
    "def unpool_op(x, stride, name='unpool'):\n",
    "\n",
    "    with tf.name_scope(name) as scope:\n",
    "\n",
    "        if stride==1:\n",
    "            return x\n",
    "\n",
    "        shape = x.get_shape().as_list()\n",
    "        dim = len(shape[1:-1])\n",
    "        out = (tf.reshape(x, [-1] + shape[-dim:]))\n",
    "        for i in range(dim, 0, -1):\n",
    "            out = tf.concat([out]*stride, i)\n",
    "        out_size = [-1] + [s * stride for s in shape[1:-1]] + [shape[-1]]\n",
    "        out = tf.reshape(out, out_size, name=scope)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_layer(x, dims, stride, train, bias=None, name='conv', activation=tf.nn.elu):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # Parameters\n",
    "        weights = tf.get_variable('w', dims,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        if bias is not None:\n",
    "            biases = tf.get_variable('b', [dims[-1]],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "\n",
    "        # Layer structure\n",
    "        if bias is None:\n",
    "             conv = tf.nn.conv2d(x, weights, strides=[1, stride, stride, 1], padding='SAME', name='conv')\n",
    "        else:\n",
    "             conv = tf.nn.bias_add(tf.nn.conv2d(x, weights, strides=[1, stride, stride, 1],\n",
    "                        padding='SAME'), biases, name='conv')\n",
    "        normalized = tf.layers.batch_normalization(conv, axis=3, training=train,\n",
    "                    name='spatial_batch_norm')        \n",
    "        activations = activation(normalized, name='activation')\n",
    "        activations = tf.layers.dropout(activations, rate=DROPOUT, training=train, name='dropout')\n",
    "\n",
    "        # Variable summaries\n",
    "        variable_summary(weights, 'weights')\n",
    "        if bias is not None:\n",
    "            variable_summary(biases, 'biases')\n",
    "        tf.summary.histogram('pre-activations', normalized)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconvolution_layer(x, dims, stride, train, bias=None, name='deconv', activation=tf.nn.elu):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # Parameters\n",
    "        weights = tf.get_variable('w', dims,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if bias is not None:\n",
    "            biases = tf.get_variable('b', [dims[-1]],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "\n",
    "        # Layer structure\n",
    "        unpool = unpool_op(x, stride, name='unpool')\n",
    "        if bias is None:\n",
    "            deconv = tf.nn.conv2d(unpool, weights, strides=[1, 1, 1, 1], padding='SAME', name='deconv')\n",
    "        else:\n",
    "            deconv = tf.nn.bias_add(tf.nn.conv2d(unpool, weights, strides=[1, 1, 1, 1],\n",
    "                        padding='SAME'), biases, name='deconv')\n",
    "        normalized = tf.layers.batch_normalization(deconv, axis=3, training=train,\n",
    "                    name='spatial_batch_norm')        \n",
    "        activations = activation(deconv, name='activation')\n",
    "        activations = tf.layers.dropout(activations, rate=DROPOUT, training=train, name='dropout')\n",
    "\n",
    "        # Variable summaries\n",
    "        variable_summary(weights, 'weights')\n",
    "        if bias is not None:\n",
    "            variable_summary(biases, 'biases')\n",
    "        tf.summary.histogram('pre-activations', deconv)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(x, dims, train, bias=None, name='fc', activation=tf.nn.elu):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # Parameters\n",
    "        weights = tf.get_variable('w', dims,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if bias is not None:\n",
    "            biases = tf.get_variable('b', [dims[-1]],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "\n",
    "        # Layer structure\n",
    "        if bias is None:\n",
    "            dense = tf.matmul(x, weights, name='dense')\n",
    "        else:\n",
    "            dense = tf.nn.bias_add(tf.matmul(x, weights), biases, name='dense')\n",
    "\n",
    "        normalized = tf.layers.batch_normalization(dense, axis=1, training=train,\n",
    "                    name='batch_norm')\n",
    "        activations = activation(normalized, name='activation')\n",
    "\n",
    "        # Variable summaries\n",
    "        variable_summary(weights, 'weights')\n",
    "        if bias is not None:\n",
    "            variable_summary(biases, 'biases')\n",
    "        tf.summary.histogram('pre-activations', normalized)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_sample(mean, stddev, name):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # mean is unconstrained; stddev must be strictly positive\n",
    "        stddev = 1e-6 + tf.nn.softplus(stddev)\n",
    "\n",
    "        # actually sample\n",
    "        z = mean + stddev * tf.random_normal(tf.shape(mean), 0, 1, dtype=tf.float32)\n",
    "\n",
    "        # Variable summaries\n",
    "        tf.summary.histogram('mean', mean)\n",
    "        tf.summary.histogram('stddev', stddev)\n",
    "        tf.summary.histogram('z', z)\n",
    "\n",
    "        return mean, stddev, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(x, label, train, name='cat'):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        cat = tf.concat([x, label], 1)\n",
    "        \n",
    "        # Variable summaries\n",
    "        tf.summary.histogram('cat', cat)\n",
    "        return cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x, xhat, mu, sigma):\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        # Structure\n",
    "        pred = tf.losses.absolute_difference(x, xhat,\n",
    "                reduction=tf.losses.Reduction.MEAN)\n",
    "\n",
    "        # offsetx = x + 0.5\n",
    "        # pred = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        #         labels=offsetx, logits=xhat))\n",
    "\n",
    "        KLdiv = 0.5 * tf.reduce_mean(tf.square(mu) + \\\n",
    "                    tf.square(sigma) - tf.log(1e-8 + tf.square(sigma)) - 1)\n",
    "\n",
    "        loss = tf.add(REGULARIZATION * KLdiv, pred)\n",
    "\n",
    "        # Summaries\n",
    "        tf.summary.scalar('prediction', pred)\n",
    "        tf.summary.scalar('prior', KLdiv)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    return loss, pred, KLdiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(img, label, train):\n",
    "\n",
    "    with tf.variable_scope('encoder'):\n",
    "\n",
    "        label = tf.to_float(label)\n",
    "        \n",
    "        # convolution\n",
    "        conv1 = convolution_layer(img, [5, 5, 1, 64], 2, train, name='conv1')\n",
    "        conv2 = convolution_layer(conv1, [5, 5, 64, 128], 2, train, name='conv2')\n",
    "        conv3 = convolution_layer(conv2, [5, 5, 128, 256], 2, train, name='conv3')\n",
    "\n",
    "        # transition\n",
    "        conv3 = tf.reshape(conv3, [-1, 32*32*256], name='reshape1')\n",
    "\n",
    "        # dense output\n",
    "        fc1_label = dense_layer(label, [15, 32], train,\n",
    "                activation=tf.identity, name='fc1_label')     \n",
    "        \n",
    "        # concatenate output with labels (CVAE)\n",
    "        cat1 = concatenate(conv3, fc1_label, train, name='cat1')\n",
    "        \n",
    "        # dense output\n",
    "        fc2_mu = dense_layer(cat1, [32*32*256+32, 32], train,\n",
    "                activation=tf.identity, name='fc2_mu')\n",
    "              \n",
    "        # dense output\n",
    "        fc3_sigma = dense_layer(conv3, [32*32*256, 32], train,\n",
    "                activation=tf.identity, name='fc3_sigma')\n",
    "        # sample\n",
    "        mu, sigma, z = gaussian_sample(fc2_mu, fc3_sigma, name='output')\n",
    "\n",
    "    return mu, sigma, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(z, label, train):\n",
    "\n",
    "    with tf.variable_scope('decoder'):\n",
    "\n",
    "        label = tf.to_float(label)\n",
    "        \n",
    "        # dense input\n",
    "        fc1_label = dense_layer(label, [15, 2*32], train, name='fc1_label')\n",
    "        \n",
    "        # concatenate output with labels (CVAE)\n",
    "        cat1 = concatenate(z, fc1_label, train, name='cat1')\n",
    "        \n",
    "        # dense input\n",
    "        fc1 = dense_layer(cat1, [32+(2*32), 32*32*256], train, name='fc1')\n",
    "\n",
    "        # transition\n",
    "        fc1 = tf.reshape(fc1, [-1, 32, 32, 256], name='reshape1')\n",
    "\n",
    "        # deconvolution\n",
    "        deconv1 = deconvolution_layer(fc1, [5, 5, 256, 128], 2, train,\n",
    "                            name='deconv1')\n",
    "        deconv2 = deconvolution_layer(deconv1, [5, 5, 128, 64], 2, train, \n",
    "                            name='deconv2')\n",
    "        deconv3 = deconvolution_layer(deconv2, [5, 5, 64, 32], 2, train,\n",
    "                            name='deconv3')\n",
    "        logits = deconvolution_layer(deconv3, [5, 5, 32, 1], 1, train,\n",
    "                            activation=tf.identity, name='logits')\n",
    "\n",
    "        # put into image range for display\n",
    "        with tf.name_scope('range'):\n",
    "            xhat = 0.5 * tf.nn.tanh(logits)\n",
    "\n",
    "    return xhat, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    datefmt=\"%Y-%m-%dT%H:%M:%S%z\",\n",
    "    format=\"%(asctime)s [train/initialize] %(levelname)-8s %(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "NOW_STR = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "RUN_DESC = \"cross-entropy loss, 256x256 images, no bias\"\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-11T23:29:09+0000 [train/initialize] INFO     initializing run: cvae/vae_004\n",
      "2018-07-11T23:29:25+0000 [train/initialize] INFO       step      loss      recon     reg\n",
      "2018-07-11T23:29:41+0000 [train/initialize] INFO          0     0.209     0.198     0.104\n",
      "2018-07-11T23:29:41+0000 [train/initialize] INFO     saving graph\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def initialize():\n",
    "\n",
    "    logging.info(\"initializing run: {}\".format(RUN_NAME))\n",
    "\n",
    "    # write a note regarding this run\n",
    "    os.makedirs(SUMMARY_DIR, exist_ok=True)\n",
    "    with open(os.path.join(SUMMARY_DIR, \"description.txt\"), 'w') as fh:\n",
    "        fh.write(NOW_STR+\" \"+RUN_DESC)\n",
    "\n",
    "    # Control memory usage\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    # Report OOM details\n",
    "    run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "    # Build graph\n",
    "    with tf.Graph().as_default() as graph:\n",
    "\n",
    "        # Repeatable results\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "        # Get Data\n",
    "        train_dataset, train_iterator = inputs(filenames=TRAIN_TFRECORDS,\n",
    "                batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, num_shards = 1, shard_index = 0)\n",
    "\n",
    "        # Data placeholder\n",
    "        data_handle = tf.placeholder(tf.string, shape=[])\n",
    "        iterator = tf.data.Iterator.from_string_handle(\n",
    "            data_handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "        \n",
    "       \n",
    "\n",
    "        filename, image, labels = iterator.get_next()\n",
    "\n",
    "        # Train/validate flag\n",
    "        train = tf.placeholder(tf.bool)\n",
    "\n",
    "        # Global counter\n",
    "        global_step = tf.train.get_or_create_global_step(graph)\n",
    "\n",
    "        # Dropout\n",
    "        dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Autoencoder\n",
    "        mu, sigma, z = encoder(image, labels, train)\n",
    "        xhat, logits = decoder(z, labels, train)\n",
    "        loss, recon, reg = evaluate(image, xhat, mu, sigma)\n",
    "\n",
    "        # Training branch - control dependencies so batchnorm params are updated\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\\\n",
    "                .minimize(loss, global_step=global_step, name='optimizer')\n",
    "\n",
    "        # Log output for Tensorboard\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_summary_logger = tf.summary.FileWriter(SUMMARY_DIR+'/train',\n",
    "                        graph=graph, flush_secs=30)\n",
    "\n",
    "        # Initializer\n",
    "        init_variables = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "        # Save state\n",
    "        tf.add_to_collection('optimizer', optimizer)\n",
    "\n",
    "        tf.add_to_collection('filename', filename)\n",
    "        tf.add_to_collection('image', image)\n",
    "        #tf.add_to_collection('view', view)\n",
    "\n",
    "        #tf.add_to_collection('gender', gender)\n",
    "        #tf.add_to_collection('age', age)\n",
    "        tf.add_to_collection('labels', labels)\n",
    "        \n",
    "        tf.add_to_collection('mu', mu)\n",
    "        \n",
    "        tf.add_to_collection('sigma', sigma)\n",
    "        tf.add_to_collection('xhat', xhat)\n",
    "\n",
    "        tf.add_to_collection('loss', loss)\n",
    "        tf.add_to_collection('recon', recon)\n",
    "        tf.add_to_collection('reg', reg)\n",
    "\n",
    "        tf.add_to_collection('data_handle', data_handle)\n",
    "        tf.add_to_collection('train', train)\n",
    "\n",
    "        tf.add_to_collection('merged', merged)\n",
    "\n",
    "        writer = tf.train.Saver()\n",
    "\n",
    "        # Run one step: this initializes the graph and saves our starting statistics\n",
    "        with tf.Session(config=config) as session:\n",
    "\n",
    "            session.run(init_variables)\n",
    "\n",
    "            train_handle = session.run(train_iterator.string_handle())\n",
    "\n",
    "            # Output header\n",
    "            logging.info(\"  step      loss      recon     reg\")\n",
    "\n",
    "            _, step = session.run([optimizer, global_step],\n",
    "                    feed_dict = { data_handle: train_handle, train: 1 },\n",
    "                    options = run_options)\n",
    "\n",
    "            loss_, recon_, reg_, summary = \\\n",
    "                session.run([loss, recon, reg, merged],\n",
    "                           feed_dict = { data_handle: train_handle, train: 0 })\n",
    "            train_summary_logger.add_summary(summary, step)\n",
    "\n",
    "            logging.info(\"{: 6d} {:9.3g} {:9.3g} {:9.3g}\".format(step, loss_, recon_, reg_))\n",
    "\n",
    "            # Save graph\n",
    "            logging.info(\"saving graph\")\n",
    "            writer.save(session, MODEL_PREFIX, global_step=step, write_meta_graph=False)\n",
    "            writer.export_meta_graph(MODEL_GRAPH)\n",
    "            \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='initialize training graph')\n",
    "    initialize()\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../data/models/cvae/vae_004/vae-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-11T23:29:49+0000 [train/initialize] INFO     Restoring parameters from ../../data/models/cvae/vae_004/vae-0\n",
      "2018-07-11T23:29:50+0000 [train/initialize] INFO       step      loss      recon     reg\n",
      "2018-07-11T23:30:37+0000 [train/initialize] INFO        100     0.365     0.346     0.194\n",
      "2018-07-11T23:31:25+0000 [train/initialize] INFO        200     0.395     0.352     0.438\n",
      "2018-07-11T23:32:14+0000 [train/initialize] INFO        300     0.381     0.352     0.286\n",
      "2018-07-11T23:33:02+0000 [train/initialize] INFO        400     0.381     0.349     0.316\n",
      "2018-07-11T23:33:51+0000 [train/initialize] INFO        500     0.373     0.336      0.37\n",
      "2018-07-11T23:34:39+0000 [train/initialize] INFO        600     0.392     0.303      0.89\n",
      "2018-07-11T23:35:27+0000 [train/initialize] INFO        700     0.363     0.283     0.801\n",
      "2018-07-11T23:36:15+0000 [train/initialize] INFO        800     0.405     0.274      1.31\n",
      "2018-07-11T23:37:04+0000 [train/initialize] INFO        900     0.365     0.259      1.06\n",
      "2018-07-11T23:37:52+0000 [train/initialize] INFO       1000     0.287     0.228     0.594\n",
      "2018-07-11T23:38:40+0000 [train/initialize] INFO       1100     0.271      0.22     0.511\n",
      "2018-07-11T23:39:29+0000 [train/initialize] INFO       1200     0.265     0.201     0.642\n",
      "2018-07-11T23:40:16+0000 [train/initialize] INFO       1300     0.215     0.152     0.628\n",
      "2018-07-11T23:41:04+0000 [train/initialize] INFO       1400     0.192      0.13     0.628\n",
      "2018-07-11T23:41:53+0000 [train/initialize] INFO       1500     0.221     0.155     0.659\n",
      "2018-07-11T23:42:42+0000 [train/initialize] INFO       1600      0.25      0.16     0.898\n",
      "2018-07-11T23:43:30+0000 [train/initialize] INFO       1700     0.218     0.162     0.561\n",
      "2018-07-11T23:44:18+0000 [train/initialize] INFO       1800     0.196     0.153      0.43\n",
      "2018-07-11T23:45:06+0000 [train/initialize] INFO       1900     0.216      0.15     0.664\n",
      "2018-07-11T23:45:54+0000 [train/initialize] INFO       2000     0.193     0.149     0.442\n",
      "2018-07-11T23:46:42+0000 [train/initialize] INFO       2100     0.198     0.152     0.462\n",
      "2018-07-11T23:47:30+0000 [train/initialize] INFO       2200     0.157     0.114     0.431\n",
      "2018-07-11T23:48:16+0000 [train/initialize] INFO       2300     0.237     0.157     0.796\n",
      "2018-07-11T23:49:04+0000 [train/initialize] INFO       2400     0.213      0.16     0.528\n",
      "2018-07-11T23:49:53+0000 [train/initialize] INFO       2500       0.2     0.164     0.365\n",
      "2018-07-11T23:50:43+0000 [train/initialize] INFO       2600     0.198     0.158     0.398\n",
      "2018-07-11T23:51:31+0000 [train/initialize] INFO       2700     0.191     0.155     0.359\n",
      "2018-07-11T23:52:20+0000 [train/initialize] INFO       2800     0.225     0.161     0.634\n",
      "2018-07-11T23:53:08+0000 [train/initialize] INFO       2900     0.209      0.16     0.496\n",
      "2018-07-11T23:53:56+0000 [train/initialize] INFO       3000     0.224     0.165     0.591\n",
      "2018-07-11T23:54:54+0000 [train/initialize] INFO       3100     0.218     0.168     0.497\n",
      "2018-07-11T23:55:43+0000 [train/initialize] INFO       3200     0.182     0.146      0.36\n",
      "2018-07-11T23:56:32+0000 [train/initialize] INFO       3300     0.218     0.164     0.541\n",
      "2018-07-11T23:57:20+0000 [train/initialize] INFO       3400     0.183     0.157     0.261\n",
      "2018-07-11T23:58:08+0000 [train/initialize] INFO       3500     0.189     0.161     0.278\n",
      "2018-07-11T23:59:02+0000 [train/initialize] INFO       3600     0.197     0.159     0.376\n",
      "2018-07-11T23:59:59+0000 [train/initialize] INFO       3700     0.197     0.175     0.224\n",
      "2018-07-12T00:00:47+0000 [train/initialize] INFO       3800     0.196     0.157     0.394\n",
      "2018-07-12T00:01:36+0000 [train/initialize] INFO       3900     0.186     0.162     0.237\n",
      "2018-07-12T00:02:25+0000 [train/initialize] INFO       4000     0.187     0.159     0.277\n",
      "2018-07-12T00:03:18+0000 [train/initialize] INFO       4100     0.194     0.148     0.457\n",
      "2018-07-12T00:04:19+0000 [train/initialize] INFO       4200     0.175     0.153     0.224\n",
      "2018-07-12T00:05:11+0000 [train/initialize] INFO       4300     0.177     0.154     0.224\n",
      "2018-07-12T00:05:59+0000 [train/initialize] INFO       4400     0.169     0.119     0.504\n",
      "2018-07-12T00:06:47+0000 [train/initialize] INFO       4500     0.141     0.118     0.225\n",
      "2018-07-12T00:07:34+0000 [train/initialize] INFO       4600     0.182     0.158     0.243\n",
      "2018-07-12T00:08:22+0000 [train/initialize] INFO       4700     0.173     0.156     0.177\n",
      "2018-07-12T00:09:11+0000 [train/initialize] INFO       4800      0.18     0.161     0.185\n",
      "2018-07-12T00:09:59+0000 [train/initialize] INFO       4900     0.193     0.159     0.344\n",
      "2018-07-12T00:11:03+0000 [train/initialize] INFO       5000     0.206      0.16     0.454\n",
      "2018-07-12T00:11:51+0000 [train/initialize] INFO       5100     0.172     0.155     0.171\n",
      "2018-07-12T00:12:41+0000 [train/initialize] INFO       5200     0.195     0.154     0.408\n",
      "2018-07-12T00:13:41+0000 [train/initialize] INFO       5300     0.184     0.163     0.219\n",
      "2018-07-12T00:14:31+0000 [train/initialize] INFO       5400     0.168     0.152     0.157\n",
      "2018-07-12T00:15:19+0000 [train/initialize] INFO       5500     0.175     0.161     0.138\n",
      "2018-07-12T00:16:09+0000 [train/initialize] INFO       5600     0.181     0.153     0.274\n",
      "2018-07-12T00:16:57+0000 [train/initialize] INFO       5700     0.183     0.169      0.14\n",
      "2018-07-12T00:17:48+0000 [train/initialize] INFO       5800     0.189     0.168     0.209\n",
      "2018-07-12T00:18:37+0000 [train/initialize] INFO       5900      0.18     0.157     0.236\n",
      "2018-07-12T00:19:26+0000 [train/initialize] INFO       6000     0.175     0.165    0.0995\n",
      "2018-07-12T00:20:14+0000 [train/initialize] INFO       6100     0.186     0.164     0.218\n",
      "2018-07-12T00:21:10+0000 [train/initialize] INFO       6200     0.184     0.171     0.135\n",
      "2018-07-12T00:22:00+0000 [train/initialize] INFO       6300     0.186     0.171     0.152\n",
      "2018-07-12T00:22:49+0000 [train/initialize] INFO       6400     0.177     0.163     0.148\n",
      "2018-07-12T00:23:41+0000 [train/initialize] INFO       6500     0.199      0.17     0.289\n",
      "2018-07-12T00:24:33+0000 [train/initialize] INFO       6600     0.172     0.157     0.147\n",
      "2018-07-12T00:25:25+0000 [train/initialize] INFO       6700     0.137     0.121      0.16\n",
      "2018-07-12T00:26:10+0000 [train/initialize] INFO       6800     0.189     0.173     0.162\n",
      "2018-07-12T00:27:02+0000 [train/initialize] INFO       6900     0.209     0.184     0.255\n",
      "2018-07-12T00:27:56+0000 [train/initialize] INFO       7000     0.175     0.161     0.145\n",
      "2018-07-12T00:28:48+0000 [train/initialize] INFO       7100     0.186     0.177    0.0905\n",
      "2018-07-12T00:29:41+0000 [train/initialize] INFO       7200     0.175     0.167    0.0825\n",
      "2018-07-12T00:30:33+0000 [train/initialize] INFO       7300     0.179     0.159     0.203\n",
      "2018-07-12T00:31:27+0000 [train/initialize] INFO       7400     0.186     0.176     0.105\n",
      "2018-07-12T00:32:23+0000 [train/initialize] INFO       7500     0.195     0.183     0.126\n",
      "2018-07-12T00:33:15+0000 [train/initialize] INFO       7600     0.178     0.161     0.171\n",
      "2018-07-12T00:34:06+0000 [train/initialize] INFO       7700     0.169     0.161    0.0756\n",
      "2018-07-12T00:34:58+0000 [train/initialize] INFO       7800     0.185     0.169     0.168\n",
      "2018-07-12T00:35:47+0000 [train/initialize] INFO       7900     0.172     0.164    0.0771\n",
      "2018-07-12T00:36:35+0000 [train/initialize] INFO       8000     0.182     0.175    0.0701\n",
      "2018-07-12T00:37:24+0000 [train/initialize] INFO       8100     0.182     0.173    0.0892\n",
      "2018-07-12T00:38:15+0000 [train/initialize] INFO       8200     0.188     0.178    0.0973\n",
      "2018-07-12T00:39:04+0000 [train/initialize] INFO       8300     0.168      0.16    0.0812\n",
      "2018-07-12T00:39:52+0000 [train/initialize] INFO       8400     0.185     0.171     0.133\n",
      "2018-07-12T00:40:41+0000 [train/initialize] INFO       8500     0.184     0.175     0.082\n",
      "2018-07-12T00:41:29+0000 [train/initialize] INFO       8600     0.173     0.165    0.0768\n",
      "2018-07-12T00:42:17+0000 [train/initialize] INFO       8700      0.18     0.174    0.0597\n",
      "2018-07-12T00:43:05+0000 [train/initialize] INFO       8800     0.182     0.177    0.0535\n",
      "2018-07-12T00:43:53+0000 [train/initialize] INFO       8900     0.148     0.143    0.0532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12T00:44:42+0000 [train/initialize] INFO       9000     0.139     0.132    0.0751\n",
      "2018-07-12T00:45:28+0000 [train/initialize] INFO       9100      0.18      0.17     0.102\n",
      "2018-07-12T00:46:16+0000 [train/initialize] INFO       9200      0.17     0.163    0.0754\n",
      "2018-07-12T00:47:04+0000 [train/initialize] INFO       9300     0.173     0.168    0.0537\n",
      "2018-07-12T00:47:52+0000 [train/initialize] INFO       9400     0.164      0.16    0.0468\n",
      "2018-07-12T00:48:39+0000 [train/initialize] INFO       9500      0.16     0.156    0.0388\n",
      "2018-07-12T00:49:28+0000 [train/initialize] INFO       9600     0.175     0.167     0.079\n",
      "2018-07-12T00:50:16+0000 [train/initialize] INFO       9700     0.184     0.177     0.073\n",
      "2018-07-12T00:51:04+0000 [train/initialize] INFO       9800     0.177     0.172    0.0484\n",
      "2018-07-12T00:51:52+0000 [train/initialize] INFO       9900     0.172     0.169    0.0358\n",
      "2018-07-12T00:52:40+0000 [train/initialize] INFO      10000     0.171     0.168    0.0308\n",
      "2018-07-12T00:53:29+0000 [train/initialize] INFO      10100      0.18     0.175    0.0485\n",
      "2018-07-12T00:54:17+0000 [train/initialize] INFO      10200      0.18     0.175     0.044\n",
      "2018-07-12T00:55:05+0000 [train/initialize] INFO      10300      0.18     0.176    0.0418\n",
      "2018-07-12T00:55:53+0000 [train/initialize] INFO      10400     0.187     0.183    0.0428\n",
      "2018-07-12T00:56:41+0000 [train/initialize] INFO      10500     0.187     0.182    0.0509\n",
      "2018-07-12T00:57:29+0000 [train/initialize] INFO      10600     0.181     0.177    0.0316\n",
      "2018-07-12T00:58:17+0000 [train/initialize] INFO      10700     0.174     0.171    0.0336\n",
      "2018-07-12T00:59:05+0000 [train/initialize] INFO      10800     0.187     0.183    0.0408\n",
      "2018-07-12T00:59:53+0000 [train/initialize] INFO      10900     0.184      0.18    0.0396\n",
      "2018-07-12T01:00:41+0000 [train/initialize] INFO      11000     0.154     0.151    0.0338\n",
      "2018-07-12T01:01:29+0000 [train/initialize] INFO      11100     0.169     0.166    0.0289\n",
      "2018-07-12T01:02:18+0000 [train/initialize] INFO      11200     0.136     0.133    0.0293\n",
      "2018-07-12T01:03:02+0000 [train/initialize] INFO      11300     0.141     0.138     0.034\n",
      "2018-07-12T01:03:50+0000 [train/initialize] INFO      11400     0.165     0.159    0.0558\n",
      "2018-07-12T01:04:37+0000 [train/initialize] INFO      11500     0.175     0.171    0.0365\n",
      "2018-07-12T01:05:28+0000 [train/initialize] INFO      11600     0.169     0.165    0.0369\n",
      "2018-07-12T01:06:16+0000 [train/initialize] INFO      11700      0.17     0.168    0.0232\n",
      "2018-07-12T01:07:04+0000 [train/initialize] INFO      11800     0.181     0.177    0.0338\n",
      "2018-07-12T01:07:52+0000 [train/initialize] INFO      11900     0.186     0.183    0.0297\n",
      "2018-07-12T01:08:40+0000 [train/initialize] INFO      12000     0.193      0.19    0.0332\n",
      "2018-07-12T01:09:29+0000 [train/initialize] INFO      12100     0.184      0.18    0.0401\n",
      "2018-07-12T01:10:19+0000 [train/initialize] INFO      12200     0.193     0.189    0.0359\n",
      "2018-07-12T01:11:08+0000 [train/initialize] INFO      12300     0.179     0.176    0.0296\n",
      "2018-07-12T01:11:56+0000 [train/initialize] INFO      12400     0.171     0.168    0.0272\n",
      "2018-07-12T01:12:44+0000 [train/initialize] INFO      12500     0.178     0.175    0.0254\n",
      "2018-07-12T01:13:32+0000 [train/initialize] INFO      12600     0.184     0.181    0.0362\n",
      "2018-07-12T01:14:20+0000 [train/initialize] INFO      12700     0.176     0.172    0.0319\n",
      "2018-07-12T01:15:08+0000 [train/initialize] INFO      12800     0.175     0.172    0.0313\n",
      "2018-07-12T01:15:56+0000 [train/initialize] INFO      12900     0.184     0.182    0.0273\n",
      "2018-07-12T01:16:45+0000 [train/initialize] INFO      13000     0.175     0.173    0.0255\n",
      "2018-07-12T01:17:33+0000 [train/initialize] INFO      13100     0.182     0.179    0.0271\n",
      "2018-07-12T01:18:21+0000 [train/initialize] INFO      13200     0.178     0.174    0.0375\n",
      "2018-07-12T01:19:09+0000 [train/initialize] INFO      13300     0.168     0.165    0.0286\n",
      "2018-07-12T01:19:58+0000 [train/initialize] INFO      13400     0.164     0.162    0.0253\n",
      "2018-07-12T01:20:46+0000 [train/initialize] INFO      13500     0.135     0.133     0.025\n",
      "2018-07-12T01:21:32+0000 [train/initialize] INFO      13600      0.17     0.165    0.0494\n",
      "2018-07-12T01:22:20+0000 [train/initialize] INFO      13700     0.185     0.182    0.0314\n",
      "2018-07-12T01:23:08+0000 [train/initialize] INFO      13800      0.18     0.177    0.0307\n",
      "2018-07-12T01:23:56+0000 [train/initialize] INFO      13900     0.169     0.166    0.0321\n",
      "2018-07-12T01:24:44+0000 [train/initialize] INFO      14000     0.168     0.165     0.031\n",
      "2018-07-12T01:25:32+0000 [train/initialize] INFO      14100     0.164     0.161    0.0322\n",
      "2018-07-12T01:26:20+0000 [train/initialize] INFO      14200     0.183      0.18    0.0284\n",
      "2018-07-12T01:27:09+0000 [train/initialize] INFO      14300     0.188     0.185    0.0336\n",
      "2018-07-12T01:27:57+0000 [train/initialize] INFO      14400      0.19     0.186    0.0349\n",
      "2018-07-12T01:28:46+0000 [train/initialize] INFO      14500     0.174     0.171    0.0255\n",
      "2018-07-12T01:29:34+0000 [train/initialize] INFO      14600     0.191     0.188    0.0339\n",
      "2018-07-12T01:30:21+0000 [train/initialize] INFO      14700     0.173     0.171    0.0223\n",
      "2018-07-12T01:31:09+0000 [train/initialize] INFO      14800     0.176     0.173    0.0308\n",
      "2018-07-12T01:31:57+0000 [train/initialize] INFO      14900     0.181     0.177    0.0339\n",
      "2018-07-12T01:32:45+0000 [train/initialize] INFO      15000     0.178     0.175    0.0284\n",
      "2018-07-12T01:33:33+0000 [train/initialize] INFO      15100     0.176     0.173    0.0253\n",
      "2018-07-12T01:34:22+0000 [train/initialize] INFO      15200     0.185     0.183    0.0261\n",
      "2018-07-12T01:35:10+0000 [train/initialize] INFO      15300     0.178     0.176     0.023\n",
      "2018-07-12T01:35:58+0000 [train/initialize] INFO      15400     0.167     0.164    0.0273\n",
      "2018-07-12T01:36:46+0000 [train/initialize] INFO      15500     0.168     0.165    0.0308\n",
      "2018-07-12T01:37:34+0000 [train/initialize] INFO      15600     0.178     0.176    0.0217\n",
      "2018-07-12T01:38:23+0000 [train/initialize] INFO      15700     0.143      0.14    0.0248\n",
      "2018-07-12T01:39:11+0000 [train/initialize] INFO      15800     0.135     0.133    0.0206\n",
      "2018-07-12T01:39:58+0000 [train/initialize] INFO      15900     0.175      0.17    0.0413\n",
      "2018-07-12T01:40:46+0000 [train/initialize] INFO      16000     0.187     0.183    0.0378\n",
      "2018-07-12T01:41:34+0000 [train/initialize] INFO      16100     0.176     0.173    0.0296\n",
      "2018-07-12T01:42:22+0000 [train/initialize] INFO      16200     0.161     0.158    0.0304\n",
      "2018-07-12T01:43:10+0000 [train/initialize] INFO      16300     0.151     0.149    0.0238\n",
      "2018-07-12T01:43:58+0000 [train/initialize] INFO      16400     0.179     0.176    0.0268\n",
      "2018-07-12T01:44:45+0000 [train/initialize] INFO      16500     0.182     0.179    0.0304\n",
      "2018-07-12T01:45:34+0000 [train/initialize] INFO      16600     0.197     0.193    0.0372\n",
      "2018-07-12T01:46:22+0000 [train/initialize] INFO      16700     0.184      0.18     0.033\n",
      "2018-07-12T01:47:10+0000 [train/initialize] INFO      16800     0.186     0.183    0.0286\n",
      "2018-07-12T01:47:59+0000 [train/initialize] INFO      16900     0.183      0.18    0.0303\n",
      "2018-07-12T01:48:46+0000 [train/initialize] INFO      17000     0.186     0.184    0.0225\n",
      "2018-07-12T01:49:34+0000 [train/initialize] INFO      17100     0.174      0.17    0.0301\n",
      "2018-07-12T01:50:22+0000 [train/initialize] INFO      17200     0.183      0.18    0.0321\n",
      "2018-07-12T01:51:10+0000 [train/initialize] INFO      17300     0.167     0.164    0.0271\n",
      "2018-07-12T01:51:58+0000 [train/initialize] INFO      17400     0.167     0.164    0.0267\n",
      "2018-07-12T01:52:46+0000 [train/initialize] INFO      17500     0.157     0.155    0.0251\n",
      "2018-07-12T01:53:34+0000 [train/initialize] INFO      17600     0.174     0.171    0.0232\n",
      "2018-07-12T01:54:23+0000 [train/initialize] INFO      17700     0.186     0.182    0.0414\n",
      "2018-07-12T01:55:11+0000 [train/initialize] INFO      17800     0.182      0.18    0.0208\n",
      "2018-07-12T01:55:59+0000 [train/initialize] INFO      17900     0.192     0.189    0.0289\n",
      "2018-07-12T01:56:48+0000 [train/initialize] INFO      18000     0.125     0.122    0.0227\n",
      "2018-07-12T01:57:31+0000 [train/initialize] INFO      18100     0.174     0.169    0.0464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12T01:58:18+0000 [train/initialize] INFO      18200      0.18     0.177    0.0318\n",
      "2018-07-12T01:59:06+0000 [train/initialize] INFO      18300     0.181     0.177    0.0422\n",
      "2018-07-12T01:59:54+0000 [train/initialize] INFO      18400     0.177     0.175    0.0292\n",
      "2018-07-12T02:00:43+0000 [train/initialize] INFO      18500     0.164     0.162    0.0253\n",
      "2018-07-12T02:01:31+0000 [train/initialize] INFO      18600     0.182     0.179     0.027\n",
      "2018-07-12T02:02:19+0000 [train/initialize] INFO      18700     0.187     0.184    0.0303\n",
      "2018-07-12T02:03:07+0000 [train/initialize] INFO      18800     0.192      0.19    0.0262\n",
      "2018-07-12T02:03:55+0000 [train/initialize] INFO      18900     0.183      0.18    0.0334\n",
      "2018-07-12T02:04:43+0000 [train/initialize] INFO      19000     0.185     0.182    0.0293\n",
      "2018-07-12T02:05:33+0000 [train/initialize] INFO      19100     0.176     0.173     0.035\n",
      "2018-07-12T02:06:21+0000 [train/initialize] INFO      19200     0.158     0.155    0.0241\n",
      "2018-07-12T02:07:09+0000 [train/initialize] INFO      19300     0.188     0.185    0.0285\n",
      "2018-07-12T02:07:58+0000 [train/initialize] INFO      19400     0.185     0.182    0.0351\n",
      "2018-07-12T02:08:46+0000 [train/initialize] INFO      19500     0.172      0.17    0.0265\n",
      "2018-07-12T02:09:34+0000 [train/initialize] INFO      19600     0.174     0.172     0.028\n",
      "2018-07-12T02:10:22+0000 [train/initialize] INFO      19700     0.185     0.182     0.034\n",
      "2018-07-12T02:11:10+0000 [train/initialize] INFO      19800     0.176     0.174    0.0216\n",
      "2018-07-12T02:11:58+0000 [train/initialize] INFO      19900     0.175     0.173    0.0252\n",
      "2018-07-12T02:12:46+0000 [train/initialize] INFO      20000     0.162     0.159    0.0284\n",
      "2018-07-12T02:13:35+0000 [train/initialize] INFO      20100     0.191     0.188    0.0293\n",
      "2018-07-12T02:14:22+0000 [train/initialize] INFO      20200     0.161     0.159    0.0179\n",
      "2018-07-12T02:15:11+0000 [train/initialize] INFO      20300     0.132      0.13    0.0247\n",
      "2018-07-12T02:15:57+0000 [train/initialize] INFO      20400     0.174      0.17    0.0396\n",
      "2018-07-12T02:16:44+0000 [train/initialize] INFO      20500     0.181     0.177    0.0391\n",
      "2018-07-12T02:17:32+0000 [train/initialize] INFO      20600     0.162     0.159    0.0306\n",
      "2018-07-12T02:18:20+0000 [train/initialize] INFO      20700     0.178     0.174     0.032\n",
      "2018-07-12T02:19:09+0000 [train/initialize] INFO      20800     0.171     0.168    0.0305\n",
      "2018-07-12T02:19:57+0000 [train/initialize] INFO      20900     0.173     0.171    0.0233\n",
      "2018-07-12T02:20:46+0000 [train/initialize] INFO      21000     0.191     0.189    0.0264\n",
      "2018-07-12T02:21:34+0000 [train/initialize] INFO      21100      0.18     0.176    0.0332\n",
      "2018-07-12T02:22:22+0000 [train/initialize] INFO      21200     0.194     0.191    0.0337\n",
      "2018-07-12T02:23:10+0000 [train/initialize] INFO      21300     0.173     0.171    0.0235\n",
      "2018-07-12T02:23:58+0000 [train/initialize] INFO      21400      0.18     0.177    0.0274\n",
      "2018-07-12T02:24:47+0000 [train/initialize] INFO      21500     0.163     0.161    0.0253\n",
      "2018-07-12T02:25:35+0000 [train/initialize] INFO      21600     0.177     0.174    0.0275\n",
      "2018-07-12T02:26:24+0000 [train/initialize] INFO      21700     0.183      0.18    0.0269\n",
      "2018-07-12T02:27:12+0000 [train/initialize] INFO      21800     0.178     0.174    0.0331\n",
      "2018-07-12T02:28:00+0000 [train/initialize] INFO      21900     0.168     0.165    0.0337\n",
      "2018-07-12T02:28:48+0000 [train/initialize] INFO      22000      0.17     0.167    0.0261\n",
      "2018-07-12T02:29:37+0000 [train/initialize] INFO      22100     0.177     0.175     0.027\n",
      "2018-07-12T02:30:25+0000 [train/initialize] INFO      22200     0.179     0.176    0.0254\n",
      "2018-07-12T02:31:13+0000 [train/initialize] INFO      22300     0.165     0.162    0.0281\n",
      "2018-07-12T02:32:03+0000 [train/initialize] INFO      22400     0.168     0.165    0.0268\n",
      "2018-07-12T02:32:51+0000 [train/initialize] INFO      22500     0.124     0.121    0.0265\n",
      "2018-07-12T02:33:39+0000 [train/initialize] INFO      22600     0.129     0.127    0.0192\n",
      "2018-07-12T02:34:26+0000 [train/initialize] INFO      22700     0.188     0.184    0.0372\n",
      "2018-07-12T02:35:14+0000 [train/initialize] INFO      22800     0.154     0.152    0.0235\n",
      "2018-07-12T02:36:02+0000 [train/initialize] INFO      22900      0.16     0.157    0.0303\n",
      "2018-07-12T02:36:50+0000 [train/initialize] INFO      23000     0.166     0.164    0.0226\n",
      "2018-07-12T02:37:38+0000 [train/initialize] INFO      23100     0.147     0.145    0.0205\n",
      "2018-07-12T02:38:26+0000 [train/initialize] INFO      23200     0.177     0.174    0.0257\n",
      "2018-07-12T02:39:14+0000 [train/initialize] INFO      23300     0.196     0.193    0.0319\n",
      "2018-07-12T02:40:02+0000 [train/initialize] INFO      23400     0.167     0.164    0.0283\n",
      "2018-07-12T02:40:50+0000 [train/initialize] INFO      23500     0.185     0.182    0.0243\n",
      "2018-07-12T02:41:38+0000 [train/initialize] INFO      23600     0.178     0.175    0.0287\n",
      "2018-07-12T02:42:26+0000 [train/initialize] INFO      23700     0.171     0.167    0.0352\n",
      "2018-07-12T02:43:15+0000 [train/initialize] INFO      23800     0.179     0.175    0.0371\n",
      "2018-07-12T02:44:02+0000 [train/initialize] INFO      23900     0.166     0.163    0.0289\n",
      "2018-07-12T02:44:50+0000 [train/initialize] INFO      24000     0.182      0.18    0.0274\n",
      "2018-07-12T02:45:38+0000 [train/initialize] INFO      24100     0.184     0.181    0.0354\n",
      "2018-07-12T02:46:26+0000 [train/initialize] INFO      24200     0.181     0.179     0.025\n",
      "2018-07-12T02:47:15+0000 [train/initialize] INFO      24300     0.174     0.171    0.0286\n",
      "2018-07-12T02:48:04+0000 [train/initialize] INFO      24400     0.162     0.159    0.0219\n",
      "2018-07-12T02:48:52+0000 [train/initialize] INFO      24500     0.177     0.173    0.0347\n",
      "2018-07-12T02:49:40+0000 [train/initialize] INFO      24600     0.172      0.17    0.0193\n",
      "2018-07-12T02:50:28+0000 [train/initialize] INFO      24700     0.176     0.174    0.0268\n",
      "2018-07-12T02:51:17+0000 [train/initialize] INFO      24800      0.14     0.137    0.0234\n",
      "2018-07-12T02:52:02+0000 [train/initialize] INFO      24900     0.184     0.179    0.0469\n",
      "2018-07-12T02:52:50+0000 [train/initialize] INFO      25000     0.183      0.18    0.0342\n",
      "2018-07-12T02:53:38+0000 [train/initialize] INFO      25100     0.184     0.181    0.0306\n",
      "2018-07-12T02:54:26+0000 [train/initialize] INFO      25200     0.159     0.156    0.0282\n",
      "2018-07-12T02:55:15+0000 [train/initialize] INFO      25300     0.167     0.163    0.0309\n",
      "2018-07-12T02:56:03+0000 [train/initialize] INFO      25400     0.169     0.166    0.0263\n",
      "2018-07-12T02:56:50+0000 [train/initialize] INFO      25500     0.178     0.174    0.0334\n",
      "2018-07-12T02:57:38+0000 [train/initialize] INFO      25600     0.184     0.181    0.0337\n",
      "2018-07-12T02:58:26+0000 [train/initialize] INFO      25700     0.177     0.174    0.0286\n",
      "2018-07-12T02:59:14+0000 [train/initialize] INFO      25800     0.176     0.173    0.0301\n",
      "2018-07-12T03:00:02+0000 [train/initialize] INFO      25900      0.18     0.176    0.0351\n",
      "2018-07-12T03:00:50+0000 [train/initialize] INFO      26000     0.167     0.164    0.0252\n",
      "2018-07-12T03:01:38+0000 [train/initialize] INFO      26100     0.162     0.159    0.0316\n",
      "2018-07-12T03:02:26+0000 [train/initialize] INFO      26200     0.161     0.159    0.0242\n",
      "2018-07-12T03:03:14+0000 [train/initialize] INFO      26300     0.175     0.172     0.029\n",
      "2018-07-12T03:04:02+0000 [train/initialize] INFO      26400     0.169     0.167    0.0258\n",
      "2018-07-12T03:04:50+0000 [train/initialize] INFO      26500     0.194     0.191    0.0278\n",
      "2018-07-12T03:05:40+0000 [train/initialize] INFO      26600     0.173      0.17    0.0293\n",
      "2018-07-12T03:06:28+0000 [train/initialize] INFO      26700     0.167     0.164      0.03\n",
      "2018-07-12T03:07:16+0000 [train/initialize] INFO      26800     0.169     0.166    0.0296\n",
      "2018-07-12T03:08:04+0000 [train/initialize] INFO      26900     0.161     0.158    0.0256\n",
      "2018-07-12T03:08:53+0000 [train/initialize] INFO      27000      0.15     0.147    0.0254\n",
      "2018-07-12T03:09:41+0000 [train/initialize] INFO      27100     0.138     0.135    0.0242\n",
      "2018-07-12T03:10:27+0000 [train/initialize] INFO      27200     0.187     0.183      0.04\n",
      "2018-07-12T03:11:15+0000 [train/initialize] INFO      27300     0.175     0.173    0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12T03:12:02+0000 [train/initialize] INFO      27400     0.172     0.169    0.0309\n",
      "2018-07-12T03:12:50+0000 [train/initialize] INFO      27500     0.164     0.162    0.0275\n",
      "2018-07-12T03:13:38+0000 [train/initialize] INFO      27600     0.156     0.153    0.0259\n",
      "2018-07-12T03:14:27+0000 [train/initialize] INFO      27700     0.172     0.168     0.034\n",
      "2018-07-12T03:15:15+0000 [train/initialize] INFO      27800     0.182     0.179    0.0353\n",
      "2018-07-12T03:16:03+0000 [train/initialize] INFO      27900     0.188     0.185    0.0321\n",
      "2018-07-12T03:16:51+0000 [train/initialize] INFO      28000     0.176     0.173    0.0266\n",
      "2018-07-12T03:17:39+0000 [train/initialize] INFO      28100     0.166     0.163    0.0309\n",
      "2018-07-12T03:18:27+0000 [train/initialize] INFO      28200     0.175     0.173    0.0272\n",
      "2018-07-12T03:19:15+0000 [train/initialize] INFO      28300     0.179     0.176    0.0285\n",
      "2018-07-12T03:20:03+0000 [train/initialize] INFO      28400      0.17     0.166    0.0344\n",
      "2018-07-12T03:20:51+0000 [train/initialize] INFO      28500      0.19     0.187    0.0345\n",
      "2018-07-12T03:21:40+0000 [train/initialize] INFO      28600     0.169     0.166    0.0288\n",
      "2018-07-12T03:22:28+0000 [train/initialize] INFO      28700     0.162     0.159    0.0325\n",
      "2018-07-12T03:23:16+0000 [train/initialize] INFO      28800     0.181     0.179    0.0252\n",
      "2018-07-12T03:24:04+0000 [train/initialize] INFO      28900     0.175     0.172    0.0284\n",
      "2018-07-12T03:24:53+0000 [train/initialize] INFO      29000     0.179     0.175    0.0347\n",
      "2018-07-12T03:25:41+0000 [train/initialize] INFO      29100     0.163     0.161    0.0236\n",
      "2018-07-12T03:26:29+0000 [train/initialize] INFO      29200     0.182     0.179    0.0263\n",
      "2018-07-12T03:27:17+0000 [train/initialize] INFO      29300      0.13     0.127    0.0268\n",
      "2018-07-12T03:28:00+0000 [train/initialize] INFO      29400     0.118     0.116    0.0239\n",
      "2018-07-12T03:28:48+0000 [train/initialize] INFO      29500      0.18     0.176    0.0343\n",
      "2018-07-12T03:29:36+0000 [train/initialize] INFO      29600     0.173      0.17    0.0326\n",
      "2018-07-12T03:30:24+0000 [train/initialize] INFO      29700     0.172     0.168    0.0369\n",
      "2018-07-12T03:31:11+0000 [train/initialize] INFO      29800     0.167     0.164    0.0303\n",
      "2018-07-12T03:32:00+0000 [train/initialize] INFO      29900     0.143     0.141    0.0191\n",
      "2018-07-12T03:32:48+0000 [train/initialize] INFO      30000     0.175     0.172    0.0273\n",
      "2018-07-12T03:33:37+0000 [train/initialize] INFO      30100     0.177     0.174    0.0273\n",
      "2018-07-12T03:34:25+0000 [train/initialize] INFO      30200     0.185     0.182    0.0327\n",
      "2018-07-12T03:35:13+0000 [train/initialize] INFO      30300     0.172     0.168    0.0345\n",
      "2018-07-12T03:36:01+0000 [train/initialize] INFO      30400     0.177     0.174    0.0263\n",
      "2018-07-12T03:36:49+0000 [train/initialize] INFO      30500     0.167     0.165    0.0264\n",
      "2018-07-12T03:37:37+0000 [train/initialize] INFO      30600     0.171     0.168     0.028\n",
      "2018-07-12T03:38:26+0000 [train/initialize] INFO      30700     0.178     0.175    0.0264\n",
      "2018-07-12T03:39:14+0000 [train/initialize] INFO      30800      0.17     0.167    0.0294\n",
      "2018-07-12T03:40:02+0000 [train/initialize] INFO      30900     0.172     0.169    0.0267\n",
      "2018-07-12T03:40:50+0000 [train/initialize] INFO      31000     0.176     0.173    0.0283\n",
      "2018-07-12T03:41:38+0000 [train/initialize] INFO      31100     0.167     0.164    0.0259\n",
      "2018-07-12T03:42:26+0000 [train/initialize] INFO      31200     0.178     0.175    0.0296\n",
      "2018-07-12T03:43:15+0000 [train/initialize] INFO      31300     0.183      0.18    0.0313\n",
      "2018-07-12T03:44:03+0000 [train/initialize] INFO      31400     0.171     0.168     0.023\n",
      "2018-07-12T03:44:51+0000 [train/initialize] INFO      31500     0.165     0.162    0.0268\n",
      "2018-07-12T03:45:39+0000 [train/initialize] INFO      31600     0.147     0.145     0.023\n",
      "2018-07-12T03:46:25+0000 [train/initialize] INFO      31700     0.187     0.182    0.0434\n",
      "2018-07-12T03:47:13+0000 [train/initialize] INFO      31800     0.169     0.166    0.0307\n",
      "2018-07-12T03:48:02+0000 [train/initialize] INFO      31900     0.176     0.173    0.0318\n",
      "2018-07-12T03:48:50+0000 [train/initialize] INFO      32000     0.175     0.171    0.0333\n",
      "2018-07-12T03:49:38+0000 [train/initialize] INFO      32100      0.16     0.157    0.0288\n",
      "2018-07-12T03:50:25+0000 [train/initialize] INFO      32200     0.169     0.166    0.0288\n",
      "2018-07-12T03:51:13+0000 [train/initialize] INFO      32300     0.161     0.158    0.0295\n",
      "2018-07-12T03:52:01+0000 [train/initialize] INFO      32400     0.171     0.167    0.0339\n",
      "2018-07-12T03:52:49+0000 [train/initialize] INFO      32500     0.174      0.17    0.0326\n",
      "2018-07-12T03:53:37+0000 [train/initialize] INFO      32600     0.183      0.18    0.0295\n",
      "2018-07-12T03:54:25+0000 [train/initialize] INFO      32700      0.17     0.167    0.0257\n",
      "2018-07-12T03:55:13+0000 [train/initialize] INFO      32800     0.167     0.164    0.0303\n",
      "2018-07-12T03:56:01+0000 [train/initialize] INFO      32900     0.177     0.173    0.0345\n",
      "2018-07-12T03:56:49+0000 [train/initialize] INFO      33000      0.17     0.167    0.0378\n",
      "2018-07-12T03:57:38+0000 [train/initialize] INFO      33100     0.173      0.17    0.0262\n",
      "2018-07-12T03:58:26+0000 [train/initialize] INFO      33200     0.161     0.158    0.0318\n",
      "2018-07-12T03:59:15+0000 [train/initialize] INFO      33300     0.176     0.173    0.0287\n",
      "2018-07-12T04:00:03+0000 [train/initialize] INFO      33400     0.181     0.178    0.0279\n",
      "2018-07-12T04:00:51+0000 [train/initialize] INFO      33500     0.169     0.167     0.027\n",
      "2018-07-12T04:01:39+0000 [train/initialize] INFO      33600      0.18     0.177    0.0352\n",
      "2018-07-12T04:02:27+0000 [train/initialize] INFO      33700     0.162     0.159    0.0272\n",
      "2018-07-12T04:03:15+0000 [train/initialize] INFO      33800     0.138     0.135    0.0285\n",
      "2018-07-12T04:04:03+0000 [train/initialize] INFO      33900     0.126     0.124    0.0234\n",
      "2018-07-12T04:04:49+0000 [train/initialize] INFO      34000     0.178     0.173    0.0445\n",
      "2018-07-12T04:05:40+0000 [train/initialize] INFO      34100     0.176     0.172    0.0385\n",
      "2018-07-12T04:06:28+0000 [train/initialize] INFO      34200     0.159     0.156     0.033\n",
      "2018-07-12T04:07:16+0000 [train/initialize] INFO      34300     0.172     0.169    0.0289\n",
      "2018-07-12T04:08:04+0000 [train/initialize] INFO      34400     0.154     0.152    0.0251\n",
      "2018-07-12T04:09:10+0000 [train/initialize] INFO      34500      0.16     0.157    0.0304\n",
      "2018-07-12T04:09:57+0000 [train/initialize] INFO      34600     0.189     0.186    0.0312\n",
      "2018-07-12T04:10:45+0000 [train/initialize] INFO      34700     0.176     0.172    0.0375\n",
      "2018-07-12T04:11:33+0000 [train/initialize] INFO      34800      0.16     0.158    0.0298\n",
      "2018-07-12T04:12:21+0000 [train/initialize] INFO      34900     0.176     0.172    0.0319\n",
      "2018-07-12T04:13:09+0000 [train/initialize] INFO      35000     0.174     0.171    0.0291\n",
      "2018-07-12T04:13:57+0000 [train/initialize] INFO      35100     0.169     0.166     0.031\n",
      "2018-07-12T04:14:46+0000 [train/initialize] INFO      35200     0.172     0.169    0.0339\n",
      "2018-07-12T04:15:34+0000 [train/initialize] INFO      35300     0.168     0.165    0.0269\n",
      "2018-07-12T04:16:22+0000 [train/initialize] INFO      35400     0.182     0.178    0.0342\n",
      "2018-07-12T04:17:11+0000 [train/initialize] INFO      35500     0.177     0.174    0.0326\n",
      "2018-07-12T04:17:59+0000 [train/initialize] INFO      35600     0.164     0.161    0.0251\n",
      "2018-07-12T04:18:47+0000 [train/initialize] INFO      35700     0.186     0.183    0.0314\n",
      "2018-07-12T04:19:35+0000 [train/initialize] INFO      35800     0.179     0.175    0.0382\n",
      "2018-07-12T04:20:23+0000 [train/initialize] INFO      35900     0.182     0.179    0.0309\n",
      "2018-07-12T04:21:11+0000 [train/initialize] INFO      36000     0.155     0.152    0.0274\n",
      "2018-07-12T04:22:00+0000 [train/initialize] INFO      36100     0.128     0.125    0.0292\n",
      "2018-07-12T04:22:43+0000 [train/initialize] INFO      36200     0.162     0.158    0.0392\n",
      "2018-07-12T04:23:31+0000 [train/initialize] INFO      36300     0.174     0.171    0.0335\n",
      "2018-07-12T04:24:19+0000 [train/initialize] INFO      36400      0.17     0.167    0.0348\n",
      "2018-07-12T04:25:07+0000 [train/initialize] INFO      36500     0.171     0.168    0.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12T04:25:54+0000 [train/initialize] INFO      36600     0.173      0.17    0.0289\n",
      "2018-07-12T04:26:42+0000 [train/initialize] INFO      36700     0.163     0.161    0.0252\n",
      "2018-07-12T04:27:30+0000 [train/initialize] INFO      36800     0.174     0.171    0.0329\n",
      "2018-07-12T04:28:19+0000 [train/initialize] INFO      36900     0.186     0.182    0.0403\n",
      "2018-07-12T04:29:07+0000 [train/initialize] INFO      37000     0.187     0.182    0.0408\n",
      "2018-07-12T04:29:55+0000 [train/initialize] INFO      37100     0.164     0.161    0.0322\n",
      "2018-07-12T04:30:42+0000 [train/initialize] INFO      37200     0.172     0.169    0.0302\n",
      "2018-07-12T04:31:31+0000 [train/initialize] INFO      37300     0.157     0.153    0.0377\n",
      "2018-07-12T04:32:19+0000 [train/initialize] INFO      37400     0.175     0.172    0.0358\n",
      "2018-07-12T04:33:07+0000 [train/initialize] INFO      37500     0.163     0.159    0.0404\n",
      "2018-07-12T04:33:56+0000 [train/initialize] INFO      37600      0.17     0.167    0.0328\n",
      "2018-07-12T04:34:44+0000 [train/initialize] INFO      37700     0.175     0.172    0.0351\n",
      "2018-07-12T04:35:32+0000 [train/initialize] INFO      37800     0.179     0.176    0.0348\n",
      "2018-07-12T04:36:20+0000 [train/initialize] INFO      37900     0.176     0.173    0.0282\n",
      "2018-07-12T04:37:08+0000 [train/initialize] INFO      38000      0.17     0.167    0.0294\n",
      "2018-07-12T04:37:56+0000 [train/initialize] INFO      38100     0.181     0.177    0.0374\n",
      "2018-07-12T04:38:44+0000 [train/initialize] INFO      38200     0.165     0.162     0.029\n",
      "2018-07-12T04:39:32+0000 [train/initialize] INFO      38300     0.167     0.163    0.0322\n",
      "2018-07-12T04:40:21+0000 [train/initialize] INFO      38400     0.128     0.125    0.0297\n",
      "2018-07-12T04:41:06+0000 [train/initialize] INFO      38500     0.171     0.166    0.0544\n",
      "2018-07-12T04:41:54+0000 [train/initialize] INFO      38600     0.164     0.161    0.0357\n",
      "2018-07-12T04:42:42+0000 [train/initialize] INFO      38700     0.178     0.174    0.0379\n",
      "2018-07-12T04:43:31+0000 [train/initialize] INFO      38800     0.161     0.158    0.0338\n",
      "2018-07-12T04:44:18+0000 [train/initialize] INFO      38900     0.147     0.144      0.03\n",
      "2018-07-12T04:45:06+0000 [train/initialize] INFO      39000     0.172     0.169    0.0318\n",
      "2018-07-12T04:45:54+0000 [train/initialize] INFO      39100      0.19     0.186    0.0376\n",
      "2018-07-12T04:46:42+0000 [train/initialize] INFO      39200     0.179     0.175    0.0382\n",
      "2018-07-12T04:47:30+0000 [train/initialize] INFO      39300     0.175     0.171    0.0328\n",
      "2018-07-12T04:48:18+0000 [train/initialize] INFO      39400     0.167     0.165    0.0294\n",
      "2018-07-12T04:49:06+0000 [train/initialize] INFO      39500     0.174     0.171    0.0289\n",
      "2018-07-12T04:49:54+0000 [train/initialize] INFO      39600     0.164     0.161    0.0297\n",
      "2018-07-12T04:50:42+0000 [train/initialize] INFO      39700     0.162     0.158    0.0368\n",
      "2018-07-12T04:51:30+0000 [train/initialize] INFO      39800     0.172     0.169    0.0362\n",
      "2018-07-12T04:52:18+0000 [train/initialize] INFO      39900     0.166     0.162    0.0369\n",
      "2018-07-12T04:53:07+0000 [train/initialize] INFO      40000     0.177     0.174     0.032\n",
      "2018-07-12T04:53:55+0000 [train/initialize] INFO      40100     0.172     0.169    0.0299\n",
      "2018-07-12T04:54:43+0000 [train/initialize] INFO      40200     0.175     0.171    0.0363\n",
      "2018-07-12T04:55:31+0000 [train/initialize] INFO      40300     0.168     0.165    0.0322\n",
      "2018-07-12T04:56:19+0000 [train/initialize] INFO      40400     0.172     0.169     0.035\n",
      "2018-07-12T04:57:08+0000 [train/initialize] INFO      40500     0.167     0.164    0.0343\n",
      "2018-07-12T04:57:56+0000 [train/initialize] INFO      40600     0.124     0.121    0.0373\n",
      "2018-07-12T04:58:43+0000 [train/initialize] INFO      40700     0.124     0.121    0.0297\n",
      "2018-07-12T04:59:31+0000 [train/initialize] INFO      40800     0.172     0.168    0.0394\n",
      "2018-07-12T05:00:19+0000 [train/initialize] INFO      40900     0.164      0.16    0.0363\n",
      "2018-07-12T05:01:07+0000 [train/initialize] INFO      41000     0.152     0.148     0.037\n",
      "2018-07-12T05:01:55+0000 [train/initialize] INFO      41100     0.158     0.155    0.0318\n",
      "2018-07-12T05:02:44+0000 [train/initialize] INFO      41200      0.15     0.147    0.0295\n",
      "2018-07-12T05:03:32+0000 [train/initialize] INFO      41300     0.178     0.175     0.033\n",
      "2018-07-12T05:04:20+0000 [train/initialize] INFO      41400     0.183     0.179    0.0419\n",
      "2018-07-12T05:05:10+0000 [train/initialize] INFO      41500     0.169     0.165    0.0445\n",
      "2018-07-12T05:05:57+0000 [train/initialize] INFO      41600     0.159     0.156    0.0371\n",
      "2018-07-12T05:06:45+0000 [train/initialize] INFO      41700     0.172     0.168    0.0394\n",
      "2018-07-12T05:07:33+0000 [train/initialize] INFO      41800     0.153     0.149    0.0364\n",
      "2018-07-12T05:08:21+0000 [train/initialize] INFO      41900     0.168     0.164    0.0379\n",
      "2018-07-12T05:09:09+0000 [train/initialize] INFO      42000     0.172     0.168    0.0373\n",
      "2018-07-12T05:09:57+0000 [train/initialize] INFO      42100     0.153      0.15    0.0358\n",
      "2018-07-12T05:10:47+0000 [train/initialize] INFO      42200     0.164     0.161    0.0341\n",
      "2018-07-12T05:11:35+0000 [train/initialize] INFO      42300     0.164      0.16    0.0363\n",
      "2018-07-12T05:12:24+0000 [train/initialize] INFO      42400     0.154      0.15    0.0392\n",
      "2018-07-12T05:13:12+0000 [train/initialize] INFO      42500      0.17     0.166    0.0376\n",
      "2018-07-12T05:14:00+0000 [train/initialize] INFO      42600     0.182     0.178     0.043\n",
      "2018-07-12T05:14:48+0000 [train/initialize] INFO      42700     0.176     0.173      0.03\n",
      "2018-07-12T05:15:36+0000 [train/initialize] INFO      42800     0.159     0.155    0.0353\n",
      "2018-07-12T05:16:25+0000 [train/initialize] INFO      42900      0.14     0.137    0.0323\n",
      "2018-07-12T05:17:11+0000 [train/initialize] INFO      43000      0.18     0.174    0.0527\n",
      "2018-07-12T05:17:59+0000 [train/initialize] INFO      43100     0.188     0.184    0.0389\n",
      "2018-07-12T05:18:47+0000 [train/initialize] INFO      43200      0.17     0.166    0.0407\n",
      "2018-07-12T05:19:35+0000 [train/initialize] INFO      43300     0.176     0.172    0.0394\n",
      "2018-07-12T05:20:23+0000 [train/initialize] INFO      43400     0.165     0.161    0.0356\n",
      "2018-07-12T05:21:12+0000 [train/initialize] INFO      43500     0.168     0.165    0.0372\n",
      "2018-07-12T05:22:00+0000 [train/initialize] INFO      43600     0.174      0.17    0.0376\n",
      "2018-07-12T05:22:48+0000 [train/initialize] INFO      43700     0.169     0.165    0.0413\n",
      "2018-07-12T05:23:36+0000 [train/initialize] INFO      43800      0.17     0.166    0.0469\n",
      "2018-07-12T05:24:24+0000 [train/initialize] INFO      43900     0.177     0.173    0.0366\n",
      "2018-07-12T05:25:12+0000 [train/initialize] INFO      44000     0.172     0.169    0.0357\n",
      "2018-07-12T05:26:01+0000 [train/initialize] INFO      44100     0.162     0.158    0.0386\n",
      "2018-07-12T05:26:49+0000 [train/initialize] INFO      44200     0.176     0.172    0.0368\n",
      "2018-07-12T05:27:36+0000 [train/initialize] INFO      44300     0.179     0.175    0.0393\n",
      "2018-07-12T05:28:24+0000 [train/initialize] INFO      44400     0.169     0.165    0.0368\n",
      "2018-07-12T05:29:13+0000 [train/initialize] INFO      44500      0.16     0.156    0.0354\n",
      "2018-07-12T05:30:01+0000 [train/initialize] INFO      44600     0.172     0.169    0.0323\n",
      "2018-07-12T05:30:48+0000 [train/initialize] INFO      44700      0.17     0.166    0.0346\n",
      "2018-07-12T05:31:37+0000 [train/initialize] INFO      44800      0.17     0.166    0.0358\n",
      "2018-07-12T05:32:25+0000 [train/initialize] INFO      44900     0.166     0.162    0.0447\n",
      "2018-07-12T05:33:14+0000 [train/initialize] INFO      45000     0.167     0.163     0.036\n",
      "2018-07-12T05:34:02+0000 [train/initialize] INFO      45100     0.144      0.14    0.0445\n",
      "2018-07-12T05:34:50+0000 [train/initialize] INFO      45200     0.144      0.14    0.0388\n",
      "2018-07-12T05:35:36+0000 [train/initialize] INFO      45300     0.156     0.151     0.045\n",
      "2018-07-12T05:36:24+0000 [train/initialize] INFO      45400     0.172     0.168     0.038\n",
      "2018-07-12T05:37:13+0000 [train/initialize] INFO      45500     0.176     0.172    0.0382\n",
      "2018-07-12T05:38:01+0000 [train/initialize] INFO      45600     0.165     0.161    0.0374\n",
      "2018-07-12T05:38:49+0000 [train/initialize] INFO      45700     0.152     0.149    0.0343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12T05:39:37+0000 [train/initialize] INFO      45800     0.183     0.179    0.0421\n",
      "2018-07-12T05:40:25+0000 [train/initialize] INFO      45900     0.185     0.181    0.0391\n",
      "2018-07-12T05:41:13+0000 [train/initialize] INFO      46000     0.194      0.19    0.0397\n",
      "2018-07-12T05:42:01+0000 [train/initialize] INFO      46100      0.18     0.176    0.0445\n",
      "2018-07-12T05:42:50+0000 [train/initialize] INFO      46200     0.163     0.159    0.0415\n",
      "2018-07-12T05:43:38+0000 [train/initialize] INFO      46300     0.167     0.163    0.0379\n",
      "2018-07-12T05:44:26+0000 [train/initialize] INFO      46400     0.169     0.165     0.045\n",
      "2018-07-12T05:45:14+0000 [train/initialize] INFO      46500     0.168     0.164    0.0401\n",
      "2018-07-12T05:46:01+0000 [train/initialize] INFO      46600     0.167     0.163    0.0395\n",
      "2018-07-12T05:46:49+0000 [train/initialize] INFO      46700     0.176     0.172    0.0403\n",
      "2018-07-12T05:47:38+0000 [train/initialize] INFO      46800     0.172     0.169    0.0349\n",
      "2018-07-12T05:48:27+0000 [train/initialize] INFO      46900     0.166     0.162    0.0367\n",
      "2018-07-12T05:49:16+0000 [train/initialize] INFO      47000     0.166     0.162    0.0358\n",
      "2018-07-12T05:50:04+0000 [train/initialize] INFO      47100     0.177     0.172    0.0505\n",
      "2018-07-12T05:50:52+0000 [train/initialize] INFO      47200     0.177     0.173    0.0368\n",
      "2018-07-12T05:51:41+0000 [train/initialize] INFO      47300     0.146     0.142     0.041\n",
      "2018-07-12T05:52:29+0000 [train/initialize] INFO      47400     0.131     0.127    0.0357\n",
      "2018-07-12T05:53:13+0000 [train/initialize] INFO      47500     0.176     0.171    0.0514\n",
      "2018-07-12T05:54:00+0000 [train/initialize] INFO      47600     0.168     0.163    0.0475\n",
      "2018-07-12T05:54:48+0000 [train/initialize] INFO      47700     0.166     0.161    0.0452\n",
      "2018-07-12T05:55:37+0000 [train/initialize] INFO      47800     0.153     0.149    0.0395\n",
      "2018-07-12T05:56:25+0000 [train/initialize] INFO      47900     0.149     0.145    0.0349\n",
      "2018-07-12T05:57:13+0000 [train/initialize] INFO      48000     0.156     0.153    0.0366\n",
      "2018-07-12T05:58:00+0000 [train/initialize] INFO      48100     0.172     0.168     0.035\n",
      "2018-07-12T05:58:48+0000 [train/initialize] INFO      48200     0.188     0.184    0.0419\n",
      "2018-07-12T05:59:36+0000 [train/initialize] INFO      48300      0.17     0.165    0.0426\n",
      "2018-07-12T06:00:24+0000 [train/initialize] INFO      48400     0.167     0.164    0.0377\n",
      "2018-07-12T06:01:13+0000 [train/initialize] INFO      48500     0.166     0.162    0.0404\n",
      "2018-07-12T06:02:01+0000 [train/initialize] INFO      48600     0.165     0.161    0.0383\n",
      "2018-07-12T06:02:49+0000 [train/initialize] INFO      48700     0.156     0.152    0.0406\n",
      "2018-07-12T06:03:37+0000 [train/initialize] INFO      48800     0.172     0.167      0.05\n",
      "2018-07-12T06:04:25+0000 [train/initialize] INFO      48900     0.161     0.157    0.0386\n",
      "2018-07-12T06:05:14+0000 [train/initialize] INFO      49000     0.162     0.158    0.0428\n",
      "2018-07-12T06:06:02+0000 [train/initialize] INFO      49100     0.157     0.153    0.0379\n",
      "2018-07-12T06:06:50+0000 [train/initialize] INFO      49200     0.172     0.168    0.0365\n",
      "2018-07-12T06:07:38+0000 [train/initialize] INFO      49300     0.179     0.175    0.0373\n",
      "2018-07-12T06:08:26+0000 [train/initialize] INFO      49400     0.162     0.157    0.0445\n",
      "2018-07-12T06:09:14+0000 [train/initialize] INFO      49500     0.165     0.162    0.0363\n",
      "2018-07-12T06:10:03+0000 [train/initialize] INFO      49600     0.164      0.16    0.0389\n",
      "2018-07-12T06:10:50+0000 [train/initialize] INFO      49700     0.127     0.124    0.0319\n",
      "2018-07-12T06:11:36+0000 [train/initialize] INFO      49800     0.178     0.173    0.0512\n",
      "2018-07-12T06:12:24+0000 [train/initialize] INFO      49900     0.178     0.173    0.0499\n",
      "2018-07-12T06:13:11+0000 [train/initialize] INFO      50000     0.171     0.167    0.0368\n",
      "2018-07-12T06:14:00+0000 [train/initialize] INFO      50100     0.161     0.157    0.0397\n",
      "2018-07-12T06:14:48+0000 [train/initialize] INFO      50200     0.165     0.162    0.0358\n",
      "2018-07-12T06:15:36+0000 [train/initialize] INFO      50300     0.178     0.174    0.0407\n",
      "2018-07-12T06:16:25+0000 [train/initialize] INFO      50400     0.172     0.168    0.0368\n",
      "2018-07-12T06:17:13+0000 [train/initialize] INFO      50500     0.167     0.163    0.0441\n",
      "2018-07-12T06:18:01+0000 [train/initialize] INFO      50600     0.177     0.172    0.0504\n",
      "2018-07-12T06:18:49+0000 [train/initialize] INFO      50700     0.166     0.162    0.0346\n",
      "2018-07-12T06:19:37+0000 [train/initialize] INFO      50800     0.172     0.168    0.0421\n",
      "2018-07-12T06:20:26+0000 [train/initialize] INFO      50900     0.164     0.161    0.0367\n",
      "2018-07-12T06:21:15+0000 [train/initialize] INFO      51000      0.17     0.165    0.0484\n",
      "2018-07-12T06:22:03+0000 [train/initialize] INFO      51100     0.176     0.172    0.0407\n",
      "2018-07-12T06:22:51+0000 [train/initialize] INFO      51200     0.166     0.162    0.0436\n",
      "2018-07-12T06:23:39+0000 [train/initialize] INFO      51300     0.173      0.17     0.034\n",
      "2018-07-12T06:24:27+0000 [train/initialize] INFO      51400     0.178     0.174    0.0387\n",
      "2018-07-12T06:25:15+0000 [train/initialize] INFO      51500     0.164      0.16    0.0392\n",
      "2018-07-12T06:26:03+0000 [train/initialize] INFO      51600     0.157     0.154    0.0344\n",
      "2018-07-12T06:26:51+0000 [train/initialize] INFO      51700     0.161     0.157    0.0409\n",
      "2018-07-12T06:27:40+0000 [train/initialize] INFO      51800     0.163     0.159    0.0351\n",
      "2018-07-12T06:28:29+0000 [train/initialize] INFO      51900     0.133     0.129    0.0458\n",
      "2018-07-12T06:29:17+0000 [train/initialize] INFO      52000     0.134      0.13    0.0404\n",
      "2018-07-12T06:30:04+0000 [train/initialize] INFO      52100     0.164     0.159    0.0465\n",
      "2018-07-12T06:30:52+0000 [train/initialize] INFO      52200     0.174      0.17    0.0431\n",
      "2018-07-12T06:31:40+0000 [train/initialize] INFO      52300     0.171     0.167    0.0408\n",
      "2018-07-12T06:32:28+0000 [train/initialize] INFO      52400     0.169     0.166     0.033\n",
      "2018-07-12T06:33:16+0000 [train/initialize] INFO      52500     0.163     0.159    0.0384\n",
      "2018-07-12T06:34:04+0000 [train/initialize] INFO      52600     0.177     0.174    0.0369\n",
      "2018-07-12T06:34:52+0000 [train/initialize] INFO      52700     0.168     0.164    0.0363\n",
      "2018-07-12T06:35:40+0000 [train/initialize] INFO      52800      0.18     0.176    0.0461\n",
      "2018-07-12T06:36:28+0000 [train/initialize] INFO      52900     0.156     0.151    0.0434\n",
      "2018-07-12T06:37:17+0000 [train/initialize] INFO      53000     0.171     0.167    0.0372\n",
      "2018-07-12T06:38:04+0000 [train/initialize] INFO      53100     0.168     0.163    0.0509\n",
      "2018-07-12T06:38:52+0000 [train/initialize] INFO      53200     0.174     0.171    0.0372\n",
      "2018-07-12T06:39:40+0000 [train/initialize] INFO      53300     0.167     0.162    0.0462\n",
      "2018-07-12T06:40:28+0000 [train/initialize] INFO      53400     0.167     0.163    0.0428\n",
      "2018-07-12T06:41:16+0000 [train/initialize] INFO      53500     0.167     0.163     0.041\n",
      "2018-07-12T06:42:04+0000 [train/initialize] INFO      53600     0.174      0.17    0.0374\n",
      "2018-07-12T06:42:52+0000 [train/initialize] INFO      53700     0.173     0.169    0.0364\n",
      "2018-07-12T06:43:40+0000 [train/initialize] INFO      53800     0.161     0.157    0.0364\n",
      "2018-07-12T06:44:28+0000 [train/initialize] INFO      53900     0.185      0.18    0.0439\n",
      "2018-07-12T06:45:17+0000 [train/initialize] INFO      54000     0.163      0.16    0.0339\n",
      "2018-07-12T06:46:05+0000 [train/initialize] INFO      54100     0.173      0.17    0.0324\n",
      "2018-07-12T06:46:53+0000 [train/initialize] INFO      54200     0.122     0.118    0.0396\n",
      "2018-07-12T06:47:38+0000 [train/initialize] INFO      54300      0.17     0.164    0.0643\n",
      "2018-07-12T06:48:25+0000 [train/initialize] INFO      54400     0.173     0.168    0.0502\n",
      "2018-07-12T06:49:14+0000 [train/initialize] INFO      54500     0.148     0.144    0.0406\n",
      "2018-07-12T06:50:02+0000 [train/initialize] INFO      54600     0.157     0.154    0.0338\n",
      "2018-07-12T06:50:50+0000 [train/initialize] INFO      54700      0.17     0.166    0.0379\n",
      "2018-07-12T06:51:38+0000 [train/initialize] INFO      54800      0.16     0.156    0.0381\n",
      "2018-07-12T06:52:26+0000 [train/initialize] INFO      54900      0.17     0.167    0.0332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12T06:53:14+0000 [train/initialize] INFO      55000     0.174      0.17    0.0424\n",
      "2018-07-12T06:54:02+0000 [train/initialize] INFO      55100     0.171     0.166    0.0508\n",
      "2018-07-12T06:54:50+0000 [train/initialize] INFO      55200     0.167     0.163    0.0394\n",
      "2018-07-12T06:55:38+0000 [train/initialize] INFO      55300     0.159     0.155    0.0377\n",
      "2018-07-12T06:56:27+0000 [train/initialize] INFO      55400     0.169     0.165    0.0395\n",
      "2018-07-12T06:57:15+0000 [train/initialize] INFO      55500     0.155     0.151    0.0421\n",
      "2018-07-12T06:58:03+0000 [train/initialize] INFO      55600     0.167     0.162    0.0442\n",
      "2018-07-12T06:58:52+0000 [train/initialize] INFO      55700     0.163     0.159    0.0354\n",
      "2018-07-12T06:59:39+0000 [train/initialize] INFO      55800     0.174      0.17    0.0395\n",
      "2018-07-12T07:00:28+0000 [train/initialize] INFO      55900     0.171     0.168    0.0375\n",
      "2018-07-12T07:01:16+0000 [train/initialize] INFO      56000      0.17     0.166     0.036\n",
      "2018-07-12T07:02:04+0000 [train/initialize] INFO      56100     0.173      0.17    0.0306\n",
      "2018-07-12T07:02:52+0000 [train/initialize] INFO      56200     0.167     0.163    0.0392\n",
      "2018-07-12T07:03:40+0000 [train/initialize] INFO      56300     0.165     0.162    0.0311\n",
      "2018-07-12T07:04:28+0000 [train/initialize] INFO      56400     0.153     0.149    0.0399\n",
      "2018-07-12T07:05:19+0000 [train/initialize] INFO      56500     0.138     0.134    0.0369\n",
      "2018-07-12T07:06:05+0000 [train/initialize] INFO      56600     0.167     0.161    0.0591\n",
      "2018-07-12T07:06:53+0000 [train/initialize] INFO      56700     0.167     0.162     0.042\n",
      "2018-07-12T07:07:41+0000 [train/initialize] INFO      56800     0.169     0.165     0.041\n",
      "2018-07-12T07:08:29+0000 [train/initialize] INFO      56900     0.161     0.158    0.0398\n",
      "2018-07-12T07:09:17+0000 [train/initialize] INFO      57000     0.167     0.163     0.039\n",
      "2018-07-12T07:10:06+0000 [train/initialize] INFO      57100      0.17     0.166    0.0376\n",
      "2018-07-12T07:10:53+0000 [train/initialize] INFO      57200     0.164     0.161     0.036\n",
      "2018-07-12T07:11:42+0000 [train/initialize] INFO      57300      0.18     0.175    0.0438\n",
      "2018-07-12T07:12:30+0000 [train/initialize] INFO      57400     0.161     0.157    0.0449\n",
      "2018-07-12T07:13:19+0000 [train/initialize] INFO      57500     0.168     0.164    0.0416\n",
      "2018-07-12T07:14:07+0000 [train/initialize] INFO      57600     0.165     0.161    0.0395\n",
      "2018-07-12T07:14:54+0000 [train/initialize] INFO      57700     0.167     0.164    0.0333\n",
      "2018-07-12T07:15:42+0000 [train/initialize] INFO      57800     0.161     0.157    0.0386\n",
      "2018-07-12T07:16:30+0000 [train/initialize] INFO      57900     0.163     0.159     0.045\n",
      "2018-07-12T07:17:19+0000 [train/initialize] INFO      58000     0.168     0.163    0.0461\n",
      "2018-07-12T07:18:07+0000 [train/initialize] INFO      58100     0.167     0.164    0.0381\n",
      "2018-07-12T07:18:55+0000 [train/initialize] INFO      58200     0.164     0.161    0.0355\n",
      "2018-07-12T07:19:43+0000 [train/initialize] INFO      58300     0.166     0.163    0.0359\n",
      "2018-07-12T07:20:32+0000 [train/initialize] INFO      58400      0.18     0.175    0.0518\n",
      "2018-07-12T07:21:20+0000 [train/initialize] INFO      58500     0.167     0.162    0.0436\n",
      "2018-07-12T07:22:08+0000 [train/initialize] INFO      58600     0.178     0.174     0.037\n",
      "2018-07-12T07:22:56+0000 [train/initialize] INFO      58700     0.121     0.117    0.0451\n",
      "2018-07-12T07:23:40+0000 [train/initialize] INFO      58800     0.121     0.118    0.0324\n",
      "2018-07-12T07:24:28+0000 [train/initialize] INFO      58900     0.175     0.171    0.0428\n",
      "2018-07-12T07:25:17+0000 [train/initialize] INFO      59000     0.165      0.16    0.0446\n",
      "2018-07-12T07:26:05+0000 [train/initialize] INFO      59100     0.166     0.161    0.0451\n",
      "2018-07-12T07:26:53+0000 [train/initialize] INFO      59200     0.154      0.15    0.0392\n",
      "2018-07-12T07:27:41+0000 [train/initialize] INFO      59300     0.169     0.165    0.0374\n",
      "2018-07-12T07:28:29+0000 [train/initialize] INFO      59400     0.188     0.185    0.0346\n",
      "2018-07-12T07:29:18+0000 [train/initialize] INFO      59500     0.177     0.172     0.042\n",
      "2018-07-12T07:30:06+0000 [train/initialize] INFO      59600     0.164      0.16    0.0461\n",
      "2018-07-12T07:30:54+0000 [train/initialize] INFO      59700     0.174     0.169    0.0457\n",
      "2018-07-12T07:31:42+0000 [train/initialize] INFO      59800     0.164      0.16    0.0401\n",
      "2018-07-12T07:32:30+0000 [train/initialize] INFO      59900     0.158     0.155    0.0321\n",
      "2018-07-12T07:33:18+0000 [train/initialize] INFO      60000     0.165     0.161    0.0375\n",
      "2018-07-12T07:34:07+0000 [train/initialize] INFO      60100     0.165     0.161    0.0443\n",
      "2018-07-12T07:34:55+0000 [train/initialize] INFO      60200      0.16     0.156    0.0421\n",
      "2018-07-12T07:35:43+0000 [train/initialize] INFO      60300     0.182     0.178    0.0392\n",
      "2018-07-12T07:36:32+0000 [train/initialize] INFO      60400     0.175     0.171    0.0384\n",
      "2018-07-12T07:37:20+0000 [train/initialize] INFO      60500      0.16     0.156    0.0382\n",
      "2018-07-12T07:38:08+0000 [train/initialize] INFO      60600     0.161     0.157    0.0373\n",
      "2018-07-12T07:38:56+0000 [train/initialize] INFO      60700     0.171     0.167    0.0389\n",
      "2018-07-12T07:39:44+0000 [train/initialize] INFO      60800     0.168     0.164    0.0391\n",
      "2018-07-12T07:40:33+0000 [train/initialize] INFO      60900     0.165     0.161    0.0372\n",
      "2018-07-12T07:41:22+0000 [train/initialize] INFO      61000     0.136     0.132    0.0338\n",
      "2018-07-12T07:42:08+0000 [train/initialize] INFO      61100     0.167     0.161    0.0548\n",
      "2018-07-12T07:42:56+0000 [train/initialize] INFO      61200     0.157     0.152    0.0494\n",
      "2018-07-12T07:43:45+0000 [train/initialize] INFO      61300     0.172     0.168    0.0397\n",
      "2018-07-12T07:44:33+0000 [train/initialize] INFO      61400     0.158     0.155    0.0391\n",
      "2018-07-12T07:45:21+0000 [train/initialize] INFO      61500     0.171     0.168    0.0373\n",
      "2018-07-12T07:46:09+0000 [train/initialize] INFO      61600     0.154      0.15    0.0367\n",
      "2018-07-12T07:46:57+0000 [train/initialize] INFO      61700      0.16     0.157    0.0389\n",
      "2018-07-12T07:47:45+0000 [train/initialize] INFO      61800     0.176     0.172    0.0462\n",
      "2018-07-12T07:48:33+0000 [train/initialize] INFO      61900     0.183     0.178    0.0547\n",
      "2018-07-12T07:49:21+0000 [train/initialize] INFO      62000     0.166     0.162    0.0402\n",
      "2018-07-12T07:50:09+0000 [train/initialize] INFO      62100     0.173     0.169    0.0397\n",
      "2018-07-12T07:50:57+0000 [train/initialize] INFO      62200      0.17     0.165    0.0458\n",
      "2018-07-12T07:51:46+0000 [train/initialize] INFO      62300     0.158     0.152    0.0527\n",
      "2018-07-12T07:52:34+0000 [train/initialize] INFO      62400     0.172     0.168    0.0424\n",
      "2018-07-12T07:53:22+0000 [train/initialize] INFO      62500      0.17     0.165    0.0451\n",
      "2018-07-12T07:54:10+0000 [train/initialize] INFO      62600     0.157     0.154    0.0365\n",
      "2018-07-12T07:54:58+0000 [train/initialize] INFO      62700     0.172     0.168    0.0362\n",
      "2018-07-12T07:55:46+0000 [train/initialize] INFO      62800     0.168     0.164    0.0392\n",
      "2018-07-12T07:56:35+0000 [train/initialize] INFO      62900     0.151     0.147    0.0421\n",
      "2018-07-12T07:57:23+0000 [train/initialize] INFO      63000     0.154      0.15    0.0428\n",
      "2018-07-12T07:58:11+0000 [train/initialize] INFO      63100     0.161     0.156    0.0415\n",
      "2018-07-12T07:59:00+0000 [train/initialize] INFO      63200     0.134      0.13    0.0421\n",
      "2018-07-12T07:59:48+0000 [train/initialize] INFO      63300     0.116     0.112    0.0368\n",
      "2018-07-12T08:00:35+0000 [train/initialize] INFO      63400     0.165      0.16    0.0506\n",
      "2018-07-12T08:01:23+0000 [train/initialize] INFO      63500     0.165      0.16    0.0491\n",
      "2018-07-12T08:02:11+0000 [train/initialize] INFO      63600     0.159     0.155    0.0397\n",
      "2018-07-12T08:02:59+0000 [train/initialize] INFO      63700     0.161     0.158    0.0355\n",
      "2018-07-12T08:03:48+0000 [train/initialize] INFO      63800     0.146     0.142    0.0411\n",
      "2018-07-12T08:04:35+0000 [train/initialize] INFO      63900     0.175     0.171    0.0402\n",
      "2018-07-12T08:05:26+0000 [train/initialize] INFO      64000      0.17     0.166    0.0412\n",
      "2018-07-12T08:06:14+0000 [train/initialize] INFO      64100     0.175      0.17    0.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-12T08:07:03+0000 [train/initialize] INFO      64200     0.157     0.152    0.0505\n",
      "2018-07-12T08:07:50+0000 [train/initialize] INFO      64300     0.174      0.17    0.0426\n",
      "2018-07-12T08:08:38+0000 [train/initialize] INFO      64400     0.166     0.161    0.0486\n",
      "2018-07-12T08:09:26+0000 [train/initialize] INFO      64500     0.168     0.164    0.0383\n",
      "2018-07-12T08:10:14+0000 [train/initialize] INFO      64600     0.163     0.159    0.0401\n",
      "2018-07-12T08:11:02+0000 [train/initialize] INFO      64700     0.154      0.15    0.0398\n",
      "2018-07-12T08:11:50+0000 [train/initialize] INFO      64800     0.165     0.161    0.0468\n",
      "2018-07-12T08:12:38+0000 [train/initialize] INFO      64900     0.157     0.153    0.0396\n",
      "2018-07-12T08:13:27+0000 [train/initialize] INFO      65000     0.171     0.167    0.0331\n",
      "2018-07-12T08:14:15+0000 [train/initialize] INFO      65100     0.164      0.16    0.0378\n",
      "2018-07-12T08:15:03+0000 [train/initialize] INFO      65200     0.162     0.157    0.0507\n",
      "2018-07-12T08:15:52+0000 [train/initialize] INFO      65300     0.169     0.165    0.0413\n",
      "2018-07-12T08:16:40+0000 [train/initialize] INFO      65400      0.17     0.166     0.039\n",
      "2018-07-12T08:17:29+0000 [train/initialize] INFO      65500     0.139     0.134    0.0469\n",
      "2018-07-12T08:18:12+0000 [train/initialize] INFO      65600     0.147     0.141    0.0543\n",
      "2018-07-12T08:19:00+0000 [train/initialize] INFO      65700     0.165      0.16    0.0467\n",
      "2018-07-12T08:19:49+0000 [train/initialize] INFO      65800      0.17     0.166    0.0416\n",
      "2018-07-12T08:20:37+0000 [train/initialize] INFO      65900     0.176     0.172     0.042\n",
      "2018-07-12T08:21:25+0000 [train/initialize] INFO      66000     0.166     0.162    0.0415\n",
      "2018-07-12T08:22:14+0000 [train/initialize] INFO      66100     0.166     0.162    0.0417\n",
      "2018-07-12T08:23:02+0000 [train/initialize] INFO      66200     0.172     0.167    0.0453\n",
      "2018-07-12T08:23:50+0000 [train/initialize] INFO      66300     0.173     0.169     0.039\n",
      "2018-07-12T08:24:38+0000 [train/initialize] INFO      66400     0.163     0.159    0.0468\n",
      "2018-07-12T08:25:26+0000 [train/initialize] INFO      66500     0.166     0.162    0.0401\n",
      "2018-07-12T08:26:14+0000 [train/initialize] INFO      66600     0.168     0.163    0.0489\n",
      "2018-07-12T08:27:03+0000 [train/initialize] INFO      66700     0.161     0.157     0.038\n",
      "2018-07-12T08:27:50+0000 [train/initialize] INFO      66800     0.165     0.159    0.0602\n",
      "2018-07-12T08:28:39+0000 [train/initialize] INFO      66900     0.177     0.172    0.0497\n",
      "2018-07-12T08:29:27+0000 [train/initialize] INFO      67000     0.162     0.158    0.0368\n",
      "2018-07-12T08:30:15+0000 [train/initialize] INFO      67100     0.162     0.158    0.0382\n",
      "2018-07-12T08:31:03+0000 [train/initialize] INFO      67200     0.171     0.167    0.0422\n",
      "2018-07-12T08:31:50+0000 [train/initialize] INFO      67300     0.169     0.165    0.0375\n",
      "2018-07-12T08:32:38+0000 [train/initialize] INFO      67400     0.158     0.155    0.0366\n",
      "2018-07-12T08:33:26+0000 [train/initialize] INFO      67500     0.171     0.166    0.0474\n",
      "2018-07-12T08:34:14+0000 [train/initialize] INFO      67600     0.159     0.155    0.0374\n",
      "2018-07-12T08:35:02+0000 [train/initialize] INFO      67700     0.153     0.148    0.0435\n",
      "2018-07-12T08:35:50+0000 [train/initialize] INFO      67800     0.136     0.132    0.0346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    \n",
    "    # Repeatable results\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    # Get Data\n",
    "    _, train_iterator = inputs(filenames=TRAIN_TFRECORDS,\n",
    "                batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, num_shards = 1, shard_index = 0)\n",
    "\n",
    "        \n",
    "    # Log output for Tensorboard\n",
    "    train_summary_logger = tf.summary.FileWriter(SUMMARY_DIR+'/train', flush_secs=30)\n",
    "\n",
    "    # Run\n",
    "    with tf.Session(config=config) as session:\n",
    "\n",
    "        # restore\n",
    "        reader = tf.train.import_meta_graph(MODEL_GRAPH)\n",
    "        reader.restore(session, tf.train.latest_checkpoint(MODEL_DIR))\n",
    "\n",
    "        # must be called after reader so that the graph is populated\n",
    "        writer = tf.train.Saver()\n",
    "\n",
    "        # get references to graph endpoints\n",
    "        global_step = tf.train.get_global_step(graph)\n",
    "\n",
    "        optimizer = tf.get_collection('optimizer')[0]\n",
    "        loss = tf.get_collection('loss')[0]\n",
    "        recon = tf.get_collection('recon')[0]\n",
    "        reg = tf.get_collection('reg')[0]\n",
    "\n",
    "        data_handle = tf.get_collection('data_handle')[0]\n",
    "        train = tf.get_collection('train')[0]\n",
    "\n",
    "        merged = tf.get_collection('merged')[0]\n",
    "\n",
    "        train_handle = session.run(train_iterator.string_handle())\n",
    "\n",
    "        # Output header\n",
    "        logging.info(\"  step      loss      recon     reg\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                _, step = session.run([optimizer, global_step], \n",
    "                            feed_dict = { data_handle: train_handle, train: 1 })\n",
    "\n",
    "                if not step%DISPLAY_EVERY:\n",
    "\n",
    "                    loss_, recon_, reg_, summary = session.run([loss, recon, reg, merged],\n",
    "                            feed_dict = { data_handle: train_handle, train: 0 })\n",
    "                    train_summary_logger.add_summary(summary, step)\n",
    "\n",
    "                    logging.info(\"{: 6d} {:9.3g} {:9.3g} {:9.3g}\".format(\n",
    "                        step, loss_, recon_, reg_))\n",
    "\n",
    "                    writer.save(session, MODEL_PREFIX, global_step=step, write_meta_graph=False)\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"done\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Try out the autoencoder by running it on some test set samples.\n",
    "\n",
    "For a collection of test images:\n",
    "\n",
    "1. Create (image, label, mu, sigma) tuples\n",
    "1. For a seed image, compute the 10 nearest images using (mu, sigma)\n",
    "1. View the nearby images and their labels, comparing them to the seed image\n",
    "\n",
    "If the VAE has worked as expected, we should find that the nearby images match the seed visually, and perhaps even match according to their labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gen_documents(tfrecords, dir_name, shard_index):\n",
    "    \n",
    "    # Store the documents\n",
    "    documents = []\n",
    "    def extend(docs, m, s, xh, f, i, v, a, g, l):\n",
    "        start_id = len(docs)\n",
    "        docs.extend([\n",
    "            {\n",
    "                'filename':f_.decode('ascii') ,\n",
    "                'id_': k + start_id,\n",
    "                'image': i_.reshape(256, 256)+0.5,\n",
    "                'view': v_.decode('ascii') ,\n",
    "                'age': a_,\n",
    "                'gender': g_.decode('ascii'),\n",
    "                'labels': l_,\n",
    "                'sigma': s_,\n",
    "                'mu': m_,\n",
    "                'xhat': y_.reshape(256, 256)+0.5\n",
    "            }\n",
    "            for k, (m_, s_, y_, f_, i_, v_, a_, g_, l_) in enumerate(zip(m, s, xh, f, i, v, a, g, l))\n",
    "        ])\n",
    "        return docs\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "\n",
    "        # Repeatable results\n",
    "        tf.set_random_seed(0)\n",
    "\n",
    "        # Get Data\n",
    "        dataset, iterator = inputs(filenames=tfrecords,\n",
    "                batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, num_shards = 1, shard_index = 0)\n",
    "        # Log output for Tensorboard\n",
    "        summary_logger = tf.summary.FileWriter(SUMMARY_DIR + dir_name, flush_secs=30)\n",
    "\n",
    "        print(SUMMARY_DIR + dir_name)\n",
    "        # Run\n",
    "        with tf.Session(config=config) as session:\n",
    "\n",
    "            # restore\n",
    "            reader = tf.train.import_meta_graph(MODEL_GRAPH)\n",
    "            reader.restore(session, tf.train.latest_checkpoint(MODEL_DIR))\n",
    "\n",
    "            # get references to graph endpoints\n",
    "            filename = tf.get_collection('filename')[0]\n",
    "            image = tf.get_collection('image')[0]\n",
    "            #view = tf.get_collection('view')[0]\n",
    "            #age = tf.get_collection('age')[0]\n",
    "            #gender = tf.get_collection('gender')[0]\n",
    "            labels = tf.get_collection('labels')[0]\n",
    "\n",
    "            mu = tf.get_collection('mu')[0]\n",
    "            sigma = tf.get_collection('sigma')[0]\n",
    "            xhat = tf.get_collection('xhat')[0]\n",
    "\n",
    "            merged = tf.get_collection('merged')[0]\n",
    "\n",
    "            data_handle = tf.get_collection('data_handle')[0]\n",
    "            train = tf.get_collection('train')[0]\n",
    "            handle = session.run(iterator.string_handle())\n",
    "            \n",
    "            step = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    mu_, sigma_, xhat_, filename_, image_, labels_, summary = \\\n",
    "                        session.run([mu, sigma, xhat, filename, image, labels, merged],\n",
    "                                feed_dict = { data_handle: handle, train: 0 })\n",
    "                    documents = extend(documents, mu_, sigma_, xhat_, filename_, image_, labels_)\n",
    "\n",
    "                    step += 1\n",
    "                    summary_logger.add_summary(summary, step)                        \n",
    "                    print(\"{}.\".format(step), end=\"\", flush=True)\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    print(\".done\")\n",
    "                    break\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET = \"golden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/logs/cvae/vae_004/golden\n",
      "INFO:tensorflow:Restoring parameters from ../../data/models/cvae/vae_004/vae-67800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-11T19:18:25+0000 [train/initialize] INFO     Restoring parameters from ../../data/models/cvae/vae_004/vae-67800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1..done\n"
     ]
    }
   ],
   "source": [
    "if SUBSET == \"golden\":\n",
    "    golden_documents = gen_documents(GOLDEN_TFRECORDS, '/golden', 1)\n",
    "elif SUBSET == \"validate\":\n",
    "    validate_documents = gen_documents(VALIDATE_TFRECORDS, '/validate')\n",
    "elif SUBSET == \"test\":\n",
    "    test_documents = gen_documents(TEST_TFRECORDS, '/test')\n",
    "elif SUBSET == \"train\":\n",
    "    train_documents = gen_documents(TRAIN_TFRECORDS, '/train', 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import scipy.misc as misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json(documents):\n",
    "    \n",
    "    for doc in documents:\n",
    "        json_item = {\n",
    "            'id':doc['id_'],\n",
    "            'filename': doc['filename'],\n",
    "            'subset': SUBSET,\n",
    "            'encodedname': doc['filename'],\n",
    "            'labels': doc['labels'].tolist(),\n",
    "            'age': int(doc['age']),\n",
    "            'gender': doc['gender'],\n",
    "            'view': doc['view'],\n",
    "            'mu':doc['mu'].tolist(),\n",
    "            'sigma':doc['sigma'].tolist()\n",
    "        }\n",
    "\n",
    "        JSON_DIR = os.path.join(EMBEDDINGS_DIR, doc['filename'][:12] + \".json\")\n",
    "        with open(JSON_DIR, 'w') as outfile:\n",
    "            json.dump(json_item, outfile, separators=(',', ':'), indent = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBSET == \"golden\":\n",
    "    gen_json(golden_documents)\n",
    "\n",
    "elif SUBSET == \"validate\":\n",
    "    gen_json(validate_documents)\n",
    "    \n",
    "elif SUBSET == \"test\":\n",
    "    gen_json(test_documents)\n",
    "    \n",
    "elif SUBSET == \"train\":\n",
    "    gen_json(train_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#save the encoded 256x256 vectors as png file\n",
    "def save_images(documents):\n",
    "    for doc in documents:\n",
    "\n",
    "        path = os.path.join(ENCODED_IMAGES_DIR, doc['filename'])\n",
    "        misc.imsave(path, doc['xhat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBSET == \"golden\":\n",
    "    save_images(golden_documents)\n",
    "\n",
    "elif SUBSET == \"validate\":\n",
    "    save_images(validate_documents)\n",
    "    \n",
    "elif SUBSET == \"test\":\n",
    "    save_images(test_documents)\n",
    "    \n",
    "elif SUBSET == \"train\":\n",
    "    save_images(train_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "271px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
