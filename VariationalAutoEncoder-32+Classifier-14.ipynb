{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Built using guidance from https://arxiv.org/pdf/1512.09300.pdf\n",
    "\n",
    "Features:\n",
    "  * Uses ELU activations\n",
    "  * Deconvolution uses upscaling unpool layer before affine operator, rather than spacing with zeros\n",
    "  * Batch normalization after each transformation\n",
    "  * Dropout layer after activation\n",
    "  * abs-sum image loss rather than cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Best result so far is 10 epochs of the first training batch, with the prior and the prediction weighted equally.\n",
    "\n",
    "Running another 10 epochs doesn't hurt the similarity results, but it does make the reconstructions worse.\n",
    "\n",
    "Not quite as good after 10 epochs with regularization=0.1 (down-weighted prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running TensorFlow version 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "print(\"running TensorFlow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control memory usage\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report OOM details\n",
    "\n",
    "run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_ROOT = '../../data/'\n",
    "\n",
    "RUN_NAME = 'vae/vae_016'\n",
    "SUMMARY_DIR = os.path.join(LOG_ROOT, 'logs', RUN_NAME)\n",
    "MODEL_DIR = os.path.join(LOG_ROOT, 'models', RUN_NAME)\n",
    "MODEL_GRAPH = os.path.join(MODEL_DIR, 'vae.meta')\n",
    "MODEL_PREFIX = os.path.join(MODEL_DIR, 'vae')\n",
    "\n",
    "DATA_SIZE = 'all_packs_14'\n",
    "EXPT_NAME = 'expt_016'\n",
    "VIS_NAME = 'vis_001'\n",
    "DATA_ROOT = '/var/data/processed'\n",
    "\n",
    "EXPT_DIR = os.path.join(DATA_ROOT, EXPT_NAME, 'data')\n",
    "TFRECORDS_DIR = os.path.join('/var/data/original/tfrecords/', DATA_SIZE)\n",
    "JSON_DIR = os.path.join('/var/data/original/labels/', \"all_data_14.json\")\n",
    "EMBEDDINGS_DIR = os.path.join(EXPT_DIR, 'embeddings')\n",
    "GOLDEN_EMBEDDINGS_DIR = os.path.join(EXPT_DIR, 'golden_embeddings')\n",
    "ENCODED_IMAGES_DIR =  os.path.join(EXPT_DIR, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_256_pattern = re.compile('^train_(?P<block_id>[0-9]{3}).tfrecords')\n",
    "validate_256_pattern = re.compile('^validate_(?P<block_id>[0-9]{3}).tfrecords')\n",
    "test_256_pattern = re.compile('^test_(?P<block_id>[0-9]{3}).tfrecords')\n",
    "#golden_256_pattern1 = re.compile('golden_(?P<block_id>[0-9]{3})_src.tfrecords')\n",
    "#golden_256_pattern2 = re.compile('golden_(?P<block_id>[0-9]{3})_dr.tfrecords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALL_TFRECORDS = os.listdir(TFRECORDS_DIR)\n",
    "def get_sorted_records(pattern, directory):\n",
    "     return [ \\\n",
    "         os.path.join(directory, _file) \\\n",
    "         for _file in \\\n",
    "         sorted([_m[0] for _m in \\\n",
    "             [pattern.match(_f) for _f in os.listdir(directory)] if _m]) \\\n",
    "     ]\n",
    "\n",
    "#GOLDEN_TFRECORDS_SRC = get_sorted_records(golden_256_pattern1, TFRECORDS_DIR)\n",
    "#GOLDEN_TFRECORDS_DR = get_sorted_records(golden_256_pattern2, TFRECORDS_DIR)\n",
    "TRAIN_TFRECORDS = get_sorted_records(train_256_pattern, TFRECORDS_DIR)\n",
    "VALIDATE_TFRECORDS = get_sorted_records(validate_256_pattern, TFRECORDS_DIR)\n",
    "TEST_TFRECORDS = get_sorted_records(test_256_pattern, TFRECORDS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/var/data/original/tfrecords/all_packs_14/train_001.tfrecords'],\n",
       " ['/var/data/original/tfrecords/all_packs_14/validate_001.tfrecords'],\n",
       " ['/var/data/original/tfrecords/all_packs_14/test_001.tfrecords'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_TFRECORDS, VALIDATE_TFRECORDS, TEST_TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(SUMMARY_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode(serialized_example):\n",
    "    '''Parses an image and label from the given `serialized_example`\n",
    "    '''\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'filename': tf.FixedLenFeature([], tf.string),\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "            #'view': tf.FixedLenFeature([], tf.string),\n",
    "            #'gender': tf.FixedLenFeature([], tf.string),\n",
    "            #'age': tf.FixedLenFeature([], tf.int64),\n",
    "            'labels': tf.FixedLenSequenceFeature( [], dtype=tf.int64, default_value=-1,allow_missing=True)\n",
    "            })\n",
    "       \n",
    "\n",
    "    # Convert from a scalar string tensor\n",
    "    filename = tf.cast(features['filename'], tf.string)\n",
    "    image = tf.decode_raw(features['image'], tf.float32)\n",
    "    #view = tf.cast(features['view'], tf.string)\n",
    "    #gender = tf.cast(features['gender'], tf.string)\n",
    "    #age = tf.cast(features['age'], tf.int32)\n",
    "    labels = tf.cast(features['labels'], tf.int32)\n",
    "    \n",
    "    return filename, image, labels\n",
    "\n",
    "def _filter(filename, image, labels):\n",
    "    \n",
    "    sub_string = tf.substr(filename,3,1)\n",
    "    return tf.equal(sub_string, \"0\")\n",
    "\n",
    "def _augment(filename, image, labels):\n",
    "    '''Placeholder for data augmentation\n",
    "    '''\n",
    "    image = tf.reshape(image, [256, 256, 1])\n",
    "    return filename, image, labels\n",
    "\n",
    "\n",
    "def _normalize(filename, image, labels):\n",
    "    '''Convert `image` from [0, 255] -> [-0.5, 0.5] floats\n",
    "    '''\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "    return filename, image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs(filenames, batch_size, num_epochs, num_shards, shard_index):\n",
    "    ''' Reads input data num_epochs times or forever if num_epochs is None\n",
    "        returns dataset, iterator pair\n",
    "    '''\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        # TFRecordDataset opens a binary file and reads one record at a time.\n",
    "        # `filename` could also be a list of filenames, which will be read in order.\n",
    "       \n",
    "        \n",
    "        dataset = tf.data.TFRecordDataset(filenames)\n",
    "        \n",
    "        # The map transformation takes a function and applies it to every element\n",
    "        # of the dataset.\n",
    "        \n",
    "        dataset = dataset.map(_decode)\n",
    "        dataset = dataset.shard(num_shards, shard_index)\n",
    "        #dataset = dataset.filter(_filter)\n",
    "        dataset = dataset.map(_augment)\n",
    "        dataset = dataset.map(_normalize)\n",
    "\n",
    "        # The shuffle transformation uses a finite-sized buffer to shuffle elements\n",
    "        # in memory. The parameter is the number of elements in the buffer. For\n",
    "        # completely uniform shuffling, set the parameter to be the same as the\n",
    "        # number of elements in the dataset.\n",
    "        dataset = dataset.shuffle(1000 + 3 * batch_size)\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    return dataset, iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "## Define label keys\n",
    "LABEL_KEYS = [\n",
    "    \"Atelectasis\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Consolidation\",\n",
    "    \"Edema\",\n",
    "    \"Effusion\",\n",
    "    \"Emphysema\",\n",
    "    \"Fibrosis\",\n",
    "    \"Hernia\",\n",
    "    \"Infiltration\",\n",
    "    \"Mass\",\n",
    "    \"Nodule\",\n",
    "    \"Pleural_Thickening\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",\n",
    "]\n",
    "\n",
    "nb_label_keys = len(LABEL_KEYS)\n",
    "label_keys_by_id = LABEL_KEYS\n",
    "ids_by_label_key = OrderedDict([(k,v) for k, v in enumerate(label_keys_by_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Atelectasis': 11535, 'Cardiomegaly': 2772, 'Consolidation': 4667, 'Edema': 2303, 'Effusion': 13307, 'Emphysema': 2516, 'Fibrosis': 1686, 'Hernia': 227, 'Infiltration': 19870, 'Mass': 5746, 'Nodule': 6323, 'Pleural_Thickening': 3385, 'Pneumonia': 1353, 'Pneumothorax': 5298}\n",
      "[0.8575714920728997, 0.9657727070677138, 0.9423741788906999, 0.9715636884476713, 0.835691707413444, 0.9689336691855583, 0.9791821010520078, 0.9971971156220675, 0.7546550106188571, 0.9290512174643157, 0.9219267051908925, 0.9582036844964686, 0.9832938213068603, 0.9345829011705438]\n"
     ]
    }
   ],
   "source": [
    "def count_labels():\n",
    "    \n",
    "    with open(JSON_DIR, 'r') as j:\n",
    "        docs = json.load(j)\n",
    "\n",
    "    label2score = {key: list([]) for key in LABEL_KEYS}\n",
    "    count = {key: 0 for key in LABEL_KEYS}    \n",
    "    total = 0\n",
    "    \n",
    "    for doc in docs:\n",
    "\n",
    "        for i,l in enumerate(doc['labels']):\n",
    "            if(l == 1):\n",
    "                label = ids_by_label_key[i]\n",
    "                count[label] +=1\n",
    "                total += 1\n",
    "    return count, total\n",
    "\n",
    "c, total = count_labels()\n",
    "print(c)\n",
    "\n",
    "WEIGHTS = []\n",
    "for key in c:\n",
    "    WEIGHTS.append((total-c.get(key))/(total))\n",
    "#weights = tf.constant(weights)\n",
    "print(WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "DROPOUT = 0.7\n",
    "REGULARIZATION1 = 0.1\n",
    "REGULARIZATION2 = 0.5\n",
    "\n",
    "DISPLAY_EVERY = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summary(x, name):\n",
    "    with tf.variable_scope(name):\n",
    "        mean = tf.reduce_mean(x)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(x))\n",
    "        tf.summary.scalar('min', tf.reduce_min(x))\n",
    "        tf.summary.histogram('histogram', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpool operation doesn't yet exist in TF\n",
    "\n",
    "def unpool_op(x, stride, name='unpool'):\n",
    "\n",
    "    with tf.name_scope(name) as scope:\n",
    "\n",
    "        if stride==1:\n",
    "            return x\n",
    "\n",
    "        shape = x.get_shape().as_list()\n",
    "        dim = len(shape[1:-1])\n",
    "        out = (tf.reshape(x, [-1] + shape[-dim:]))\n",
    "        for i in range(dim, 0, -1):\n",
    "            out = tf.concat([out]*stride, i)\n",
    "        out_size = [-1] + [s * stride for s in shape[1:-1]] + [shape[-1]]\n",
    "        out = tf.reshape(out, out_size, name=scope)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_layer(x, dims, stride, train, bias=None, name='conv', activation=tf.nn.elu):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # Parameters\n",
    "        weights = tf.get_variable('w', dims,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        if bias is not None:\n",
    "            biases = tf.get_variable('b', [dims[-1]],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "\n",
    "        # Layer structure\n",
    "        if bias is None:\n",
    "             conv = tf.nn.conv2d(x, weights, strides=[1, stride, stride, 1], padding='SAME', name='conv')\n",
    "        else:\n",
    "             conv = tf.nn.bias_add(tf.nn.conv2d(x, weights, strides=[1, stride, stride, 1],\n",
    "                        padding='SAME'), biases, name='conv')\n",
    "        normalized = tf.layers.batch_normalization(conv, axis=3, training=train,\n",
    "                    name='spatial_batch_norm')        \n",
    "        activations = activation(normalized, name='activation')\n",
    "        activations = tf.layers.dropout(activations, rate=DROPOUT, training=train, name='dropout')\n",
    "\n",
    "        # Variable summaries\n",
    "        variable_summary(weights, 'weights')\n",
    "        if bias is not None:\n",
    "            variable_summary(biases, 'biases')\n",
    "        tf.summary.histogram('pre-activations', normalized)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconvolution_layer(x, dims, stride, train, bias=None, name='deconv', activation=tf.nn.elu):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # Parameters\n",
    "        weights = tf.get_variable('w', dims,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if bias is not None:\n",
    "            biases = tf.get_variable('b', [dims[-1]],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "\n",
    "        # Layer structure\n",
    "        unpool = unpool_op(x, stride, name='unpool')\n",
    "        if bias is None:\n",
    "            deconv = tf.nn.conv2d(unpool, weights, strides=[1, 1, 1, 1], padding='SAME', name='deconv')\n",
    "        else:\n",
    "            deconv = tf.nn.bias_add(tf.nn.conv2d(unpool, weights, strides=[1, 1, 1, 1],\n",
    "                        padding='SAME'), biases, name='deconv')\n",
    "        normalized = tf.layers.batch_normalization(deconv, axis=3, training=train,\n",
    "                    name='spatial_batch_norm')        \n",
    "        activations = activation(deconv, name='activation')\n",
    "        activations = tf.layers.dropout(activations, rate=DROPOUT, training=train, name='dropout')\n",
    "\n",
    "        # Variable summaries\n",
    "        variable_summary(weights, 'weights')\n",
    "        if bias is not None:\n",
    "            variable_summary(biases, 'biases')\n",
    "        tf.summary.histogram('pre-activations', deconv)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(x, dims, train, bias=None, name='fc', activation=tf.nn.elu):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # Parameters\n",
    "        weights = tf.get_variable('w', dims,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if bias is not None:\n",
    "            biases = tf.get_variable('b', [dims[-1]],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "\n",
    "        # Layer structure\n",
    "        if bias is None:\n",
    "            dense = tf.matmul(x, weights, name='dense')\n",
    "        else:\n",
    "            dense = tf.nn.bias_add(tf.matmul(x, weights), biases, name='dense')\n",
    "\n",
    "        normalized = tf.layers.batch_normalization(dense, axis=1, training=train,\n",
    "                    name='batch_norm')\n",
    "        activations = activation(normalized, name='activation')\n",
    "\n",
    "        # Variable summaries\n",
    "        variable_summary(weights, 'weights')\n",
    "        if bias is not None:\n",
    "            variable_summary(biases, 'biases')\n",
    "        tf.summary.histogram('pre-activations', normalized)\n",
    "        tf.summary.histogram('activations', activations)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_sample(mean, stddev, name):\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # mean is unconstrained; stddev must be strictly positive\n",
    "        stddev = 1e-6 + tf.nn.softplus(stddev)\n",
    "\n",
    "        # actually sample\n",
    "        z = mean + stddev * tf.random_normal(tf.shape(mean), 0, 1, dtype=tf.float32)\n",
    "\n",
    "        # Variable summaries\n",
    "        tf.summary.histogram('mean', mean)\n",
    "        tf.summary.histogram('stddev', stddev)\n",
    "        tf.summary.histogram('z', z)\n",
    "\n",
    "        return mean, stddev, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x, xhat, mu, sigma, logits_14, labels, weights):\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        # Structure\n",
    "        pred = tf.losses.absolute_difference(x, xhat,\n",
    "                reduction=tf.losses.Reduction.MEAN)\n",
    "\n",
    "        # offsetx = x + 0.5\n",
    "        # pred = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        #         labels=offsetx, logits=xhat))\n",
    "\n",
    "        KLdiv = 0.5 * tf.reduce_mean(tf.square(mu) + \\\n",
    "                    tf.square(sigma) - tf.log(1e-8 + tf.square(sigma)) - 1)\n",
    "        \n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #ce = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits_14, labels = labels))\n",
    "        #ce =  tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=logits_14, weights=(9*labels+1))\n",
    "        #ce = tf.reduce_sum(tf.nn.weighted_cross_entropy_with_logits(logits=logits_14, targets=labels, pos_weight=weights))\n",
    "        #ce =  tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=logits_14)\n",
    "        #ce =  tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=logits_14, weights=x)\n",
    "        \n",
    "        weighted_logits = tf.multiply(logits_14, weights) # shape [batch_size, 14]\n",
    "        ce = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = weighted_logits, labels = labels))\n",
    "        \n",
    "        \n",
    "        loss = tf.add(REGULARIZATION1 * KLdiv, pred)\n",
    "        loss = tf.add(loss, REGULARIZATION2 * ce)\n",
    "\n",
    "        #loss = tf.add(REGULARIZATION * KLdiv, pred)\n",
    "        # Summaries\n",
    "        tf.summary.scalar('prediction', pred)\n",
    "        tf.summary.scalar('prior', KLdiv)\n",
    "        tf.summary.scalar('cross_entropy', ce)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    return loss, pred, KLdiv, ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(img, train):\n",
    "\n",
    "    with tf.variable_scope('encoder'):\n",
    "\n",
    "        # convolution\n",
    "        conv1 = convolution_layer(img, [5, 5, 1, 64], 2, train, name='conv1')\n",
    "        conv2 = convolution_layer(conv1, [5, 5, 64, 128], 2, train, name='conv2')\n",
    "        conv3 = convolution_layer(conv2, [5, 5, 128, 256], 2, train, name='conv3')\n",
    "\n",
    "        # transition\n",
    "        conv3 = tf.reshape(conv3, [-1, 32*32*256], name='reshape1')\n",
    "\n",
    "        # dense output\n",
    "        fc1 = dense_layer(conv3, [32*32*256, 64], train,\n",
    "                activation=tf.identity, name='fc1')\n",
    "\n",
    "        # sample\n",
    "        mu, sigma, z = gaussian_sample(fc1[:, :32], fc1[:, 32:], name='output')\n",
    "\n",
    "    return mu, sigma, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(z, train):\n",
    "    \n",
    "    with tf.variable_scope('transfrom'):\n",
    "        \n",
    "        \n",
    "        dense1 = dense_layer(z, [32, 32], train,\n",
    "                activation=tf.identity, name='fc2')\n",
    "        logits = dense_layer(dense1, [32, 14], train,\n",
    "                activation=tf.identity, name='fc4')\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(z, train):\n",
    "\n",
    "    with tf.variable_scope('decoder'):\n",
    "\n",
    "        # dense input\n",
    "        fc1 = dense_layer(z, [32, 32*32*256], train, name='fc1')\n",
    "\n",
    "        # transition\n",
    "        fc1 = tf.reshape(fc1, [-1, 32, 32, 256], name='reshape1')\n",
    "\n",
    "        # deconvolution\n",
    "        deconv1 = deconvolution_layer(fc1, [5, 5, 256, 128], 2, train,\n",
    "                            name='deconv1')\n",
    "        deconv2 = deconvolution_layer(deconv1, [5, 5, 128, 64], 2, train, \n",
    "                            name='deconv2')\n",
    "        deconv3 = deconvolution_layer(deconv2, [5, 5, 64, 32], 2, train,\n",
    "                            name='deconv3')\n",
    "        logits = deconvolution_layer(deconv3, [5, 5, 32, 1], 1, train,\n",
    "                            activation=tf.identity, name='logits')\n",
    "\n",
    "        # put into image range for display\n",
    "        with tf.name_scope('range'):\n",
    "            xhat = 0.5 * tf.nn.tanh(logits)\n",
    "\n",
    "    return xhat, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_alternative(z, train):\n",
    "\n",
    "    with tf.variable_scope('decoder'):\n",
    "\n",
    "        # dense input\n",
    "        fc1 = dense_layer(z, [32, 32*32*256], train, name='fc1')\n",
    "\n",
    "        # transition\n",
    "        fc1 = tf.reshape(fc1, [-1, 32, 32, 256], name='reshape1')\n",
    "\n",
    "        # deconvolution\n",
    "        deconv1 = deconvolution_layer(fc1, [5, 5, 256, 128], 2, train,\n",
    "                            name='deconv1')\n",
    "        deconv1a = deconvolution_layer(deconv1, [5, 5, 128, 128], 2, train,\n",
    "                            name='deconv1a')\n",
    "        deconv2 = deconvolution_layer(deconv1a, [5, 5, 128, 64], 2, train, \n",
    "                            name='deconv2')\n",
    "        deconv3 = deconvolution_layer(deconv2, [5, 5, 64, 32], 2, train,\n",
    "                            name='deconv3')\n",
    "        logits = deconvolution_layer(deconv3, [5, 5, 32, 1], 1, train,\n",
    "                            activation=tf.identity, name='logits')\n",
    "\n",
    "        # put into image range for display\n",
    "        with tf.name_scope('range'):\n",
    "            xhat = 0.5 * tf.nn.tanh(logits)\n",
    "\n",
    "    return xhat, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    datefmt=\"%Y-%m-%dT%H:%M:%S%z\",\n",
    "    format=\"%(asctime)s [train/initialize] %(levelname)-8s %(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "NOW_STR = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "RUN_DESC = \"cross-entropy loss, 256x256 images, no bias\"\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12T18:03:14+0000 [train/initialize] INFO     initializing run: vae/vae_016\n",
      "2018-10-12T18:03:19+0000 [train/initialize] INFO       step      loss      recon     reg     ce\n",
      "2018-10-12T18:03:26+0000 [train/initialize] INFO          0     0.215     0.205     0.104     0.698\n",
      "2018-10-12T18:03:26+0000 [train/initialize] INFO     saving graph\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def initialize():\n",
    "\n",
    "    logging.info(\"initializing run: {}\".format(RUN_NAME))\n",
    "\n",
    "    # write a note regarding this run\n",
    "    os.makedirs(SUMMARY_DIR, exist_ok=True)\n",
    "    with open(os.path.join(SUMMARY_DIR, \"description.txt\"), 'w') as fh:\n",
    "        fh.write(NOW_STR+\" \"+RUN_DESC)\n",
    "\n",
    "    # Control memory usage\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    # Report OOM details\n",
    "    run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "    # Build graph\n",
    "    with tf.Graph().as_default() as graph:\n",
    "\n",
    "        # Repeatable results\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "        # Get Data\n",
    "        train_dataset, train_iterator = inputs(filenames=TRAIN_TFRECORDS,\n",
    "                batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, num_shards = 1, shard_index = 0)\n",
    "\n",
    "        # Data placeholder\n",
    "        data_handle = tf.placeholder(tf.string, shape=[])\n",
    "        iterator = tf.data.Iterator.from_string_handle(\n",
    "            data_handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "        \n",
    "        \n",
    "\n",
    "        filename, image, labels = iterator.get_next()\n",
    "        weights = tf.constant(WEIGHTS)\n",
    "        # Train/validate flag\n",
    "        train1 = tf.placeholder(tf.bool)\n",
    "        train2 = tf.placeholder(tf.bool)\n",
    "\n",
    "        # Global counter\n",
    "        global_step = tf.train.get_or_create_global_step(graph)\n",
    "\n",
    "        # Dropout\n",
    "        dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Autoencoder\n",
    "        mu, sigma, z = encoder(image, train1)\n",
    "        pred_labels = transform(mu, train2)\n",
    "        xhat, logits = decoder(z, train1)\n",
    "        loss, recon, reg, ce = evaluate(image, xhat, mu, sigma, pred_labels, labels, weights)\n",
    "        \n",
    "        # Training branch - control dependencies so batchnorm params are updated\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\\\n",
    "                .minimize(loss, global_step=global_step, name='optimizer')\n",
    "\n",
    "        # Log output for Tensorboard\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_summary_logger = tf.summary.FileWriter(SUMMARY_DIR+'/train',\n",
    "                        graph=graph, flush_secs=30)\n",
    "\n",
    "        # Initializer\n",
    "        init_variables = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "        # Save state\n",
    "        tf.add_to_collection('optimizer', optimizer)\n",
    "\n",
    "        tf.add_to_collection('filename', filename)\n",
    "        tf.add_to_collection('image', image)\n",
    "        tf.add_to_collection('labels', labels)\n",
    "        tf.add_to_collection('pred_labels', pred_labels)\n",
    "\n",
    "        \n",
    "        tf.add_to_collection('mu', mu)\n",
    "        \n",
    "        tf.add_to_collection('sigma', sigma)\n",
    "        tf.add_to_collection('xhat', xhat)\n",
    "\n",
    "        tf.add_to_collection('loss', loss)\n",
    "        tf.add_to_collection('recon', recon)\n",
    "        tf.add_to_collection('reg', reg)\n",
    "        tf.add_to_collection('ce', ce)\n",
    "\n",
    "        tf.add_to_collection('data_handle', data_handle)\n",
    "        tf.add_to_collection('train1', train1)\n",
    "        tf.add_to_collection('train2', train2)\n",
    "\n",
    "\n",
    "        tf.add_to_collection('merged', merged)\n",
    "\n",
    "        writer = tf.train.Saver()\n",
    "\n",
    "        # Run one step: this initializes the graph and saves our starting statistics\n",
    "        with tf.Session(config=config) as session:\n",
    "\n",
    "            session.run(init_variables)\n",
    "\n",
    "            train_handle = session.run(train_iterator.string_handle())\n",
    "\n",
    "            # Output header\n",
    "            logging.info(\"  step      loss      recon     reg     ce\")\n",
    "\n",
    "            _, step = session.run([optimizer, global_step],\n",
    "                    feed_dict = { data_handle: train_handle, train1: 0 , train2: 1},\n",
    "                    options = run_options)\n",
    "\n",
    "            loss_, recon_, reg_, ce_, summary = \\\n",
    "                session.run([loss, recon, reg, ce, merged],\n",
    "                           feed_dict = { data_handle: train_handle, train1: 0 , train2: 0})\n",
    "            train_summary_logger.add_summary(summary, step)\n",
    "\n",
    "            logging.info(\"{: 6d} {:9.3g} {:9.3g} {:9.3g} {:9.3g}\".format(step, loss_, recon_, reg_, ce_))\n",
    "\n",
    "            # Save graph\n",
    "            logging.info(\"saving graph\")\n",
    "            writer.save(session, MODEL_PREFIX, global_step=step, write_meta_graph=False)\n",
    "            writer.export_meta_graph(MODEL_GRAPH)\n",
    "            \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='initialize training graph')\n",
    "    initialize()\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../data/models/vae/vae_016/vae-15400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12T20:09:22+0000 [train/initialize] INFO     Restoring parameters from ../../data/models/vae/vae_016/vae-15400\n",
      "2018-10-12T20:09:23+0000 [train/initialize] INFO       step      loss      recon     reg         ce\n",
      "2018-10-12T20:10:01+0000 [train/initialize] INFO      15500      0.11    0.0942     0.158     0.807\n",
      "2018-10-12T20:10:38+0000 [train/initialize] INFO      15600     0.109    0.0916     0.173     0.843\n",
      "2018-10-12T20:11:23+0000 [train/initialize] INFO      15700     0.105    0.0865      0.19     0.845\n",
      "2018-10-12T20:12:06+0000 [train/initialize] INFO      15800     0.109    0.0896     0.199      0.82\n",
      "2018-10-12T20:12:49+0000 [train/initialize] INFO      15900     0.112    0.0932     0.187     0.809\n",
      "2018-10-12T20:13:33+0000 [train/initialize] INFO      16000     0.111    0.0918     0.193     0.773\n",
      "2018-10-12T20:14:17+0000 [train/initialize] INFO      16100     0.107    0.0883     0.191       0.8\n",
      "2018-10-12T20:15:00+0000 [train/initialize] INFO      16200     0.111    0.0932     0.181     0.769\n",
      "2018-10-12T20:15:44+0000 [train/initialize] INFO      16300     0.103    0.0855     0.174     0.815\n",
      "2018-10-12T20:16:29+0000 [train/initialize] INFO      16400    0.0961    0.0784     0.177     0.766\n",
      "2018-10-12T20:17:10+0000 [train/initialize] INFO      16500     0.106    0.0873     0.188     0.775\n",
      "2018-10-12T20:17:54+0000 [train/initialize] INFO      16600    0.0947    0.0754     0.193     0.771\n",
      "2018-10-12T20:18:38+0000 [train/initialize] INFO      16700     0.101    0.0829     0.183     0.793\n",
      "2018-10-12T20:19:22+0000 [train/initialize] INFO      16800    0.0989    0.0786     0.203     0.794\n",
      "2018-10-12T20:19:59+0000 [train/initialize] INFO      16900     0.109    0.0862     0.227     0.826\n",
      "2018-10-12T20:20:36+0000 [train/initialize] INFO      17000     0.104    0.0829     0.215      0.81\n",
      "2018-10-12T20:21:15+0000 [train/initialize] INFO      17100     0.108    0.0894     0.185     0.778\n",
      "2018-10-12T20:21:55+0000 [train/initialize] INFO      17200     0.108    0.0888     0.193     0.789\n",
      "2018-10-12T20:22:35+0000 [train/initialize] INFO      17300     0.109    0.0907     0.184     0.774\n",
      "2018-10-12T20:23:17+0000 [train/initialize] INFO      17400     0.104    0.0843     0.197     0.788\n",
      "2018-10-12T20:23:57+0000 [train/initialize] INFO      17500     0.087    0.0686     0.184     0.808\n",
      "2018-10-12T20:24:40+0000 [train/initialize] INFO      17600     0.101    0.0804     0.203     0.796\n",
      "2018-10-12T20:25:18+0000 [train/initialize] INFO      17700    0.0976    0.0763     0.213      0.81\n",
      "2018-10-12T20:25:57+0000 [train/initialize] INFO      17800    0.0957    0.0754     0.203     0.768\n",
      "2018-10-12T20:26:37+0000 [train/initialize] INFO      17900     0.114    0.0915     0.224     0.789\n",
      "2018-10-12T20:27:18+0000 [train/initialize] INFO      18000    0.0994    0.0791     0.203     0.776\n",
      "2018-10-12T20:27:56+0000 [train/initialize] INFO      18100     0.101    0.0804      0.21     0.822\n",
      "2018-10-12T20:28:35+0000 [train/initialize] INFO      18200     0.104    0.0825     0.214     0.793\n",
      "2018-10-12T20:29:19+0000 [train/initialize] INFO      18300     0.104    0.0836     0.205     0.819\n",
      "2018-10-12T20:29:59+0000 [train/initialize] INFO      18400     0.103    0.0823     0.208     0.785\n",
      "2018-10-12T20:30:37+0000 [train/initialize] INFO      18500    0.0927    0.0741     0.185     0.805\n",
      "2018-10-12T20:31:19+0000 [train/initialize] INFO      18600     0.104    0.0836     0.209     0.824\n",
      "2018-10-12T20:31:59+0000 [train/initialize] INFO      18700    0.0986    0.0788     0.198     0.786\n",
      "2018-10-12T20:32:37+0000 [train/initialize] INFO      18800    0.0981    0.0768     0.213     0.795\n",
      "2018-10-12T20:33:15+0000 [train/initialize] INFO      18900    0.0988    0.0768      0.22     0.807\n",
      "2018-10-12T20:33:58+0000 [train/initialize] INFO      19000    0.0954    0.0747     0.206     0.808\n",
      "2018-10-12T20:34:42+0000 [train/initialize] INFO      19100    0.0933    0.0724     0.209     0.791\n",
      "2018-10-12T20:35:26+0000 [train/initialize] INFO      19200     0.102    0.0804     0.219     0.783\n",
      "2018-10-12T20:36:10+0000 [train/initialize] INFO      19300     0.101      0.08     0.206     0.802\n",
      "2018-10-12T20:36:50+0000 [train/initialize] INFO      19400     0.104     0.084       0.2     0.787\n",
      "2018-10-12T20:37:31+0000 [train/initialize] INFO      19500    0.0965    0.0763     0.202     0.771\n",
      "2018-10-12T20:38:12+0000 [train/initialize] INFO      19600    0.0866    0.0693     0.173     0.787\n",
      "2018-10-12T20:38:55+0000 [train/initialize] INFO      19700    0.0996     0.079     0.206     0.788\n",
      "2018-10-12T20:39:39+0000 [train/initialize] INFO      19800    0.0937    0.0737       0.2     0.798\n",
      "2018-10-12T20:40:22+0000 [train/initialize] INFO      19900     0.104    0.0833     0.212     0.792\n",
      "2018-10-12T20:41:02+0000 [train/initialize] INFO      20000    0.0995    0.0793     0.202     0.765\n",
      "2018-10-12T20:41:39+0000 [train/initialize] INFO      20100    0.0983    0.0782       0.2     0.799\n",
      "2018-10-12T20:42:16+0000 [train/initialize] INFO      20200    0.0956    0.0752     0.204     0.784\n",
      "2018-10-12T20:42:53+0000 [train/initialize] INFO      20300     0.101    0.0815     0.194     0.797\n",
      "2018-10-12T20:43:32+0000 [train/initialize] INFO      20400     0.104    0.0817     0.225     0.782\n",
      "2018-10-12T20:44:11+0000 [train/initialize] INFO      20500    0.0977    0.0777       0.2     0.806\n",
      "2018-10-12T20:44:50+0000 [train/initialize] INFO      20600    0.0896    0.0705     0.191     0.806\n",
      "2018-10-12T20:45:30+0000 [train/initialize] INFO      20700    0.0985    0.0775     0.211     0.798\n",
      "2018-10-12T20:46:10+0000 [train/initialize] INFO      20800    0.0969    0.0765     0.204     0.791\n",
      "2018-10-12T20:46:49+0000 [train/initialize] INFO      20900    0.0983    0.0775     0.207     0.767\n",
      "2018-10-12T20:47:29+0000 [train/initialize] INFO      21000     0.096    0.0755     0.205      0.82\n",
      "2018-10-12T20:48:09+0000 [train/initialize] INFO      21100     0.102    0.0806     0.211     0.822\n",
      "2018-10-12T20:48:49+0000 [train/initialize] INFO      21200    0.0995    0.0788     0.208     0.803\n",
      "2018-10-12T20:49:27+0000 [train/initialize] INFO      21300     0.102    0.0805     0.212      0.81\n",
      "2018-10-12T20:50:07+0000 [train/initialize] INFO      21400     0.107    0.0879     0.195     0.785\n",
      "2018-10-12T20:50:51+0000 [train/initialize] INFO      21500     0.103    0.0834     0.197     0.776\n",
      "2018-10-12T20:53:30+0000 [train/initialize] INFO      21900    0.0938    0.0736     0.202      0.79\n",
      "2018-10-12T20:54:10+0000 [train/initialize] INFO      22000    0.0985    0.0776     0.209     0.793\n",
      "2018-10-12T20:54:51+0000 [train/initialize] INFO      22100    0.0997     0.079     0.207      0.79\n",
      "2018-10-12T20:55:34+0000 [train/initialize] INFO      22200    0.0971    0.0767     0.204     0.806\n",
      "2018-10-12T20:56:18+0000 [train/initialize] INFO      22300     0.097    0.0766     0.205     0.794\n",
      "2018-10-12T20:57:02+0000 [train/initialize] INFO      22400     0.107    0.0851     0.216     0.846\n",
      "2018-10-12T20:57:42+0000 [train/initialize] INFO      22500       0.1    0.0803     0.198     0.796\n",
      "2018-10-12T20:58:24+0000 [train/initialize] INFO      22600    0.0949    0.0747     0.202       0.8\n",
      "2018-10-12T20:59:06+0000 [train/initialize] INFO      22700    0.0864    0.0678     0.186      0.78\n",
      "2018-10-12T20:59:49+0000 [train/initialize] INFO      22800    0.0991    0.0785     0.206     0.815\n",
      "2018-10-12T21:00:33+0000 [train/initialize] INFO      22900    0.0966    0.0761     0.204     0.792\n",
      "2018-10-12T21:01:16+0000 [train/initialize] INFO      23000     0.099    0.0773     0.217     0.807\n",
      "2018-10-12T21:01:59+0000 [train/initialize] INFO      23100     0.104    0.0832      0.21     0.824\n",
      "2018-10-12T21:02:37+0000 [train/initialize] INFO      23200     0.103    0.0813     0.215     0.752\n",
      "2018-10-12T21:03:17+0000 [train/initialize] INFO      23300    0.0948    0.0736     0.212     0.815\n",
      "2018-10-12T21:03:55+0000 [train/initialize] INFO      23400    0.0973    0.0756     0.217     0.796\n",
      "2018-10-12T21:04:35+0000 [train/initialize] INFO      23500     0.107    0.0863     0.205     0.774\n",
      "2018-10-12T21:05:24+0000 [train/initialize] INFO      23600    0.0981    0.0794     0.187     0.783\n",
      "2018-10-12T21:06:06+0000 [train/initialize] INFO      23700     0.093    0.0726     0.204     0.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-12T21:06:44+0000 [train/initialize] INFO      23800    0.0839    0.0653     0.186     0.797\n",
      "2018-10-12T21:07:28+0000 [train/initialize] INFO      23900    0.0978    0.0751     0.227     0.817\n",
      "2018-10-12T21:08:13+0000 [train/initialize] INFO      24000    0.0929    0.0719      0.21     0.804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5284\u001b[0m           default) as g, context.graph_mode():\n\u001b[0;32m-> 5285\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5286\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-6ab70fb0f8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                 _, step = session.run([optimizer, global_step], \n\u001b[0;32m---> 47\u001b[0;31m                             feed_dict = { data_handle: train_handle, train1: 0 , train2: 1})\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-6ab70fb0f8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exec_type, exec_value, exec_tb)\u001b[0m\n\u001b[1;32m   1602\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1604\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1606\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_CloseSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    \n",
    "    # Repeatable results\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    # Get Data\n",
    "    train_dataset, train_iterator = inputs(filenames=TRAIN_TFRECORDS,\n",
    "            batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, num_shards = 1, shard_index = 0)\n",
    "        \n",
    "    # Log output for Tensorboard\n",
    "    train_summary_logger = tf.summary.FileWriter(SUMMARY_DIR+'/train', flush_secs=30)\n",
    "\n",
    "    # Run\n",
    "    with tf.Session(config=config) as session:\n",
    "\n",
    "        # restore\n",
    "        reader = tf.train.import_meta_graph(MODEL_GRAPH)\n",
    "        reader.restore(session, tf.train.latest_checkpoint(MODEL_DIR))\n",
    "\n",
    "        # must be called after reader so that the graph is populated\n",
    "        writer = tf.train.Saver()\n",
    "\n",
    "        # get references to graph endpoints\n",
    "        global_step = tf.train.get_global_step(graph)\n",
    "\n",
    "        optimizer = tf.get_collection('optimizer')[0]\n",
    "        loss = tf.get_collection('loss')[0]\n",
    "        recon = tf.get_collection('recon')[0]\n",
    "        reg = tf.get_collection('reg')[0]\n",
    "        ce = tf.get_collection('ce')[0]\n",
    "\n",
    "        data_handle = tf.get_collection('data_handle')[0]\n",
    "        train1 = tf.get_collection('train1')[0]\n",
    "        train2 = tf.get_collection('train2')[0]\n",
    "\n",
    "\n",
    "        merged = tf.get_collection('merged')[0]\n",
    "\n",
    "        train_handle = session.run(train_iterator.string_handle())\n",
    "\n",
    "        # Output header\n",
    "        logging.info(\"  step      loss      recon     reg         ce\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                _, step = session.run([optimizer, global_step], \n",
    "                            feed_dict = { data_handle: train_handle, train1: 0 , train2: 1})\n",
    "\n",
    "                if not step%DISPLAY_EVERY:\n",
    "\n",
    "                    loss_, recon_, reg_, ce_, summary = session.run([loss, recon, reg, ce, merged],\n",
    "                            feed_dict = { data_handle: train_handle, train1: 0 , train2: 0})\n",
    "                    train_summary_logger.add_summary(summary, step)\n",
    "\n",
    "                    logging.info(\"{: 6d} {:9.3g} {:9.3g} {:9.3g} {:9.3g}\".format(step, loss_, recon_, reg_, ce_))\n",
    "\n",
    "                    writer.save(session, MODEL_PREFIX, global_step=step, write_meta_graph=False)\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"done\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Try out the autoencoder by running it on some test set samples.\n",
    "\n",
    "For a collection of test images:\n",
    "\n",
    "1. Create (image, label, mu, sigma) tuples\n",
    "1. For a seed image, compute the 10 nearest images using (mu, sigma)\n",
    "1. View the nearby images and their labels, comparing them to the seed image\n",
    "\n",
    "If the VAE has worked as expected, we should find that the nearby images match the seed visually, and perhaps even match according to their labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gen_documents(tfrecords, dir_name, shard_index):\n",
    "    \n",
    "    # Store the documents\n",
    "    documents = []\n",
    "    def extend(docs, m, s, xh, f, i, l, p):\n",
    "        start_id = len(docs)\n",
    "        docs.extend([\n",
    "            {\n",
    "                'filename':f_.decode('ascii') ,\n",
    "                'id_': k + start_id,\n",
    "                'image': i_.reshape(256, 256)+0.5,\n",
    "                'labels': l_,\n",
    "                'pred_labels': p_,\n",
    "                'sigma': s_,\n",
    "                'mu': m_,\n",
    "                'xhat': y_.reshape(256, 256)+0.5\n",
    "            }\n",
    "            for k, (m_, s_, y_, f_, i_,  l_, p_) in enumerate(zip(m, s, xh, f, i, l, p))\n",
    "        ])\n",
    "        return docs\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "\n",
    "        # Repeatable results\n",
    "        tf.set_random_seed(0)\n",
    "\n",
    "        # Get Data\n",
    "        dataset, iterator = inputs(filenames = tfrecords, batch_size=BATCH_SIZE, num_epochs=1, num_shards=1, shard_index = 0)\n",
    "        # Log output for Tensorboard\n",
    "        summary_logger = tf.summary.FileWriter(SUMMARY_DIR + dir_name, flush_secs=30)\n",
    "\n",
    "        print(SUMMARY_DIR + dir_name)\n",
    "        # Run\n",
    "        with tf.Session(config=config) as session:\n",
    "\n",
    "            # restore\n",
    "            reader = tf.train.import_meta_graph(MODEL_GRAPH)\n",
    "            reader.restore(session, tf.train.latest_checkpoint(MODEL_DIR))\n",
    "\n",
    "            # get references to graph endpoints\n",
    "            filename = tf.get_collection('filename')[0]\n",
    "            images = tf.get_collection('image')[0]\n",
    "            labels = tf.get_collection('labels')[0]\n",
    "            pred_labels = tf.get_collection('pred_labels')[0]\n",
    "\n",
    "            mu = tf.get_collection('mu')[0]\n",
    "            sigma = tf.get_collection('sigma')[0]\n",
    "            xhat = tf.get_collection('xhat')[0]\n",
    "\n",
    "            merged = tf.get_collection('merged')[0]\n",
    "\n",
    "            data_handle = tf.get_collection('data_handle')[0]\n",
    "            train = tf.get_collection('train')[0]\n",
    "            handle = session.run(iterator.string_handle())\n",
    "            \n",
    "            step = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    mu_, sigma_, xhat_, filename_, images_, labels_, pred_labels_, summary = \\\n",
    "                        session.run([mu, sigma, xhat, filename, images, labels, pred_labels, merged],\n",
    "                                feed_dict = { data_handle: handle, train: 0 })\n",
    "                    documents = extend(documents, mu_, sigma_, xhat_, filename_, images_, labels_, pred_labels_)\n",
    "                    step += 1\n",
    "                    summary_logger.add_summary(summary, step)                        \n",
    "                    print(\"{}.\".format(step), end=\"\", flush=True)\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    print(\".done\")\n",
    "                    break\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET = \"golden_dr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/logs/vae/vae_007/golden\n",
      "INFO:tensorflow:Restoring parameters from ../../data/models/vae/vae_007/vae-11300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-02T19:00:16+0000 [train/initialize] INFO     Restoring parameters from ../../data/models/vae/vae_007/vae-11300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.3.4..done\n"
     ]
    }
   ],
   "source": [
    "if SUBSET == \"golden_dr\":\n",
    "    golden_dr_documents = gen_documents(GOLDEN_TFRECORDS_DR, '/golden', 1)\n",
    "if SUBSET == \"golden_src\":\n",
    "    golden_src_documents = gen_documents(GOLDEN_TFRECORDS_SRC, '/golden', 1)\n",
    "elif SUBSET == \"validate\":\n",
    "    validate_documents = gen_documents(VALIDATE_TFRECORDS, '/validate')\n",
    "elif SUBSET == \"test\":\n",
    "    test_documents = gen_documents(TEST_TFRECORDS, '/test')\n",
    "elif SUBSET == \"train\":\n",
    "    train_documents = gen_documents(TRAIN_TFRECORDS, '/train', 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00018562_003.png\n",
      "[1 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
      "[-1.0720762  -0.87022126 -0.83928794 -0.89940995 -0.48253763 -1.0618614\n",
      " -1.0444778  -1.102079   -0.18380427 -0.79615617 -1.0838246  -0.4062531\n",
      " -1.0027723  -0.99242157 -0.83510464]\n",
      "00012681_005.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-0.86500937 -1.221415   -0.9388064  -0.91931546 -0.83375335 -1.1990775\n",
      " -0.99700654 -1.0121012  -0.9131926  -1.1462178  -0.92739207 -0.5020547\n",
      " -1.0355849  -1.1243302  -0.927076  ]\n",
      "00012625_008.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-0.8392162  -0.9658433  -0.9229138  -1.12591    -0.64069045 -0.8182095\n",
      " -0.9011565  -1.0855676  -1.184177   -0.86749274 -1.0675727  -0.10186093\n",
      " -0.9949252  -1.0366486  -0.9219177 ]\n",
      "00012681_009.png\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.57942396 -1.1269436  -0.8235245  -0.89944476 -0.70706785 -1.1780941\n",
      " -1.2847226  -0.9287429  -0.70435673 -1.1891603  -0.9602176  -0.72983325\n",
      " -1.084122   -1.0870798  -1.121264  ]\n",
      "00004085_010.png\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.0110269  -1.0217295  -1.1565052  -0.99690515 -0.86096984 -1.1898836\n",
      " -1.1084317  -0.9620998  -1.2030429  -1.1776308  -0.71964693 -0.05626385\n",
      " -1.0158669  -0.9854034  -1.1294762 ]\n",
      "00009292_003.png\n",
      "[1 0 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.8625464  -1.0020112  -0.8543363  -1.0891466  -0.77851856 -1.1485636\n",
      " -1.1989076  -1.1076938  -0.35305488 -0.98141044 -1.1546034  -0.4321764\n",
      " -1.071051   -1.2884815  -0.6810156 ]\n",
      "00012681_018.png\n",
      "[0 0 0 1 0 0 0 0 1 0 0 0 0 0 1]\n",
      "[-0.9026039  -1.215122   -1.0283841  -1.127006   -1.0243976  -1.1978716\n",
      " -0.96867114 -0.97863877 -0.83933514 -1.0682116  -1.1489446   0.49966758\n",
      " -1.0644888  -1.1159298  -0.9912147 ]\n",
      "00016823_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.3113691  -1.1252267  -1.1000031  -1.0942876  -1.582002   -1.2783974\n",
      " -1.232116   -1.0445083  -1.440303   -1.1496077  -0.9678553   0.61527044\n",
      " -1.067638   -1.1728723  -1.2003239 ]\n",
      "00006435_000.png\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.0524913  -1.0774704  -0.9429935  -1.017452   -0.96204025 -1.1459109\n",
      " -1.0792927  -0.9100591  -0.7153461  -1.1696402  -0.82985014 -0.23465066\n",
      " -1.138831   -1.1559764  -1.0711285 ]\n",
      "00029048_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.0315928  -1.0087422  -0.9734564  -0.91007376 -0.933951   -1.0218587\n",
      " -1.1625642  -1.0257283  -0.34745377 -1.1400789  -1.2869874  -0.4207523\n",
      " -1.1784208  -1.154235   -0.9656141 ]\n",
      "00001044_000.png\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.3736973  -1.1714736  -1.2384257  -1.3573076  -1.398206   -1.2490638\n",
      " -1.0575318  -1.0916288  -1.402932   -0.83023065 -1.0379715   0.93396705\n",
      " -1.2525846  -1.0238914  -1.2484365 ]\n",
      "00027485_002.png\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "[-1.0734006  -1.259579   -1.1757983  -0.77574563 -1.0963147  -1.1208653\n",
      " -1.0031266  -1.0097872  -1.3311622  -1.1376363  -0.805318    0.2505576\n",
      " -1.0435195  -0.8869739  -0.9551166 ]\n",
      "00012625_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-0.9418211  -1.1296757  -1.2102078  -0.9769496  -1.2706866  -1.076504\n",
      " -1.1525137  -0.9517785  -1.1100551  -1.1389384  -0.96717083  0.03824172\n",
      " -0.980306   -1.1353186  -1.0396781 ]\n",
      "00013968_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "[-1.487586   -1.1732165  -1.3257529  -1.3183364  -1.8245428  -1.1005647\n",
      " -0.86674416 -1.1044992  -1.6554832  -0.98694515 -0.83902144  1.0279281\n",
      " -0.9850899  -0.8174187  -1.2185726 ]\n",
      "00012681_042.png\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.3191838  -0.86177224 -0.9009423  -1.0638108  -1.2647085  -0.7344072\n",
      " -1.2579373  -1.0697978  -1.018392   -1.1476116  -1.3963118   0.40031278\n",
      " -1.2029707  -1.0376959  -1.1067097 ]\n",
      "00028918_001.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.4003665  -1.173626   -1.5144391  -0.97737974 -1.3667818  -1.1715374\n",
      " -0.8677535  -1.0019877  -1.0845118  -1.3292392  -1.0695744   0.720344\n",
      " -1.038502   -1.1075059  -1.2191877 ]\n",
      "00017113_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-0.95972455 -1.0523211  -1.0912341  -1.083608   -1.1416734  -1.0648154\n",
      " -1.0569966  -1.0079544  -1.1577921  -1.0542059  -1.1591947   0.3663119\n",
      " -1.0926766  -1.0019948  -0.9250075 ]\n",
      "00012681_046.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.1160891  -1.0247118  -1.0636182  -0.96891814 -0.53186315 -1.107127\n",
      " -0.89306897 -1.0433465  -0.6447733  -0.83812445 -0.7110579  -0.66443664\n",
      " -1.0378766  -0.9551944  -0.7835752 ]\n",
      "00001248_022.png\n",
      "[0 0 1 0 1 0 0 0 0 0 0 0 1 0 1]\n",
      "[-0.95358914 -1.00682    -1.0462159  -1.0213938  -0.89313644 -0.9896596\n",
      " -0.9172386  -0.91412884 -0.6954174  -1.3100822  -1.0557976  -0.415716\n",
      " -1.0849395  -1.2291557  -0.9575245 ]\n",
      "00001248_026.png\n",
      "[1 0 0 0 1 0 0 0 0 0 0 0 1 0 1]\n",
      "[-1.1321396  -0.90194    -0.9856646  -1.0046978  -1.0178413  -1.0069124\n",
      " -1.1009058  -1.1981903  -0.90367377 -1.011553   -1.1343025  -0.22682051\n",
      " -0.9557115  -1.0732359  -0.89252263]\n",
      "00011193_002.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-0.75424254 -1.0079114  -0.94140476 -1.1511472  -0.8233185  -1.0421652\n",
      " -1.163271   -1.1203468  -1.1674101  -1.0475925  -1.1116503   0.26913005\n",
      " -0.96755594 -1.0125034  -1.2303399 ]\n",
      "00001248_017.png\n",
      "[1 0 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
      "[-0.9953377  -0.90344644 -1.0015081  -1.0116228  -0.9019087  -0.88302577\n",
      " -1.00338    -0.99408305 -0.82639635 -0.9045157  -1.1819003  -0.09396158\n",
      " -1.0347954  -0.9927714  -1.0101125 ]\n",
      "00012681_031.png\n",
      "[0 0 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      "[-1.2400423  -1.0347668  -1.121551   -0.960135   -0.6754732  -1.0557932\n",
      " -0.94658244 -1.1728796  -0.5841949  -1.0613315  -1.1355771   0.44388932\n",
      " -1.1639938  -1.265604   -0.9145458 ]\n",
      "00021806_008.png\n",
      "[1 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.7362666  -0.93831074 -1.0567633  -0.9785365  -0.8345275  -1.157448\n",
      " -1.1463528  -1.0332069  -0.6212361  -1.1333573  -1.1488602  -0.4042982\n",
      " -1.0714442  -0.8650394  -0.96428543]\n",
      "00001787_008.png\n",
      "[0 0 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.9131087  -1.1673229  -0.8849865  -0.998531   -0.9757318  -1.0629098\n",
      " -1.1823626  -0.9587045  -0.8489834  -0.85287344 -0.9960325  -0.37997842\n",
      " -1.1543111  -1.0073292  -1.0310762 ]\n",
      "00022492_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-0.66299164 -1.0774791  -0.9658602  -1.0723386  -0.66587245 -1.0135775\n",
      " -1.0429208  -1.0869927  -1.5001681  -0.88319993 -0.82780635 -0.11762945\n",
      " -1.0551608  -1.0034909  -1.0890325 ]\n",
      "00012681_030.png\n",
      "[1 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.3652465  -1.0075278  -1.1576794  -1.0541997  -0.88627774 -1.0275005\n",
      " -0.999236   -0.9929814  -0.42456192 -1.0795982  -0.89121234 -0.7111583\n",
      " -1.002114   -1.228421   -0.70287   ]\n",
      "00013968_008.png\n",
      "[0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[-0.869021   -1.0942092  -0.96404886 -1.1926425  -0.889802   -0.974111\n",
      " -1.0919503  -1.088371   -0.71673393 -1.0411801  -1.128594    0.20391595\n",
      " -1.0499212  -1.2491745  -0.75537723]\n",
      "00012681_039.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.1325649  -0.85971135 -1.1405987  -1.0410576  -0.8299681  -1.1634331\n",
      " -0.8699722  -1.1042056  -0.68242365 -1.0195949  -1.0192261  -0.17108215\n",
      " -0.82557935 -1.0959646  -0.80473983]\n",
      "00016424_002.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.244143   -1.046644   -1.1273978  -1.1562146  -1.078672   -1.0150039\n",
      " -0.9067582  -1.0841784  -0.87454116 -0.8442376  -1.1426817   0.40849894\n",
      " -1.1677645  -1.0131198  -1.0602967 ]\n",
      "00029579_013.png\n",
      "[0 0 1 0 1 0 0 0 0 1 0 0 0 0 0]\n",
      "[-0.74514616 -0.89598674 -1.2197102  -0.7010005  -0.75657153 -1.1175557\n",
      " -0.95074415 -1.0952687  -0.45219725 -1.1641327  -1.1073575  -0.2820782\n",
      " -0.74402934 -1.1204664  -0.9817191 ]\n",
      "00012681_000.png\n",
      "[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.3938112  -1.1172338  -1.2098552  -1.341332   -1.5272583  -1.3890233\n",
      " -1.0061823  -1.1493138  -1.280513   -1.1452612  -0.88962185  0.9327537\n",
      " -1.0037493  -1.2556505  -0.9844728 ]\n",
      "00011193_001.png\n",
      "[1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.9497076  -1.0549405  -1.0473747  -0.7712176  -0.68337065 -1.1344775\n",
      " -1.0720143  -0.9595606  -0.4078619  -1.0444646  -0.9774844  -0.44707662\n",
      " -1.0817938  -1.0273039  -1.0491072 ]\n",
      "00029418_000.png\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.71604395 -1.1142956  -0.75114304 -0.97641534 -0.94339156 -0.97043544\n",
      " -1.3381196  -1.0163013  -0.9873919  -1.1738099  -1.0208042  -0.163411\n",
      " -1.1240352  -1.079566   -1.0885289 ]\n",
      "00025035_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.2900264  -1.0212958  -1.0422897  -1.1481166  -1.481702   -0.9537669\n",
      " -1.1838934  -1.2879164  -1.3917915  -1.0717124  -1.0783489   0.52553624\n",
      " -1.1742191  -0.90153646 -1.2190154 ]\n",
      "00013968_006.png\n",
      "[0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[-0.9144014  -1.050339   -0.9442471  -1.0861267  -1.001793   -0.970444\n",
      " -0.9694899  -1.0702469  -0.49327055 -0.8928654  -1.2583759  -0.13515623\n",
      " -0.880009   -1.0520488  -0.9699453 ]\n",
      "00001434_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.2210993  -1.1919478  -1.1004682  -0.95604587 -0.92671514 -1.1508874\n",
      " -0.8055614  -0.86792517 -1.0201155  -1.2127767  -0.82239926 -0.5981397\n",
      " -1.0991399  -1.2275399  -0.91279507]\n",
      "00027485_000.png\n",
      "[0 0 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      "[-0.8506921  -0.9496451  -0.9760572  -1.2135211  -1.0214045  -1.0596007\n",
      " -1.0519402  -1.065659   -0.87750363 -0.8486573  -1.1048344   0.17535253\n",
      " -1.0000408  -0.91536176 -1.093295  ]\n",
      "00012681_015.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-0.8265833  -0.8964093  -0.865144   -0.8036076  -0.56582206 -1.0686269\n",
      " -1.1303033  -1.1058742  -0.17630082 -1.0233943  -1.0857623  -0.53594637\n",
      " -1.0047772  -1.0256784  -1.0952089 ]\n",
      "00012625_006.png\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.8216908  -0.9998856  -1.0060942  -1.0889472  -0.7059172  -0.9603893\n",
      " -1.1119688  -1.0125772  -0.6749385  -0.92888165 -1.1667564  -0.3570037\n",
      " -1.0972861  -1.1451778  -0.9001783 ]\n",
      "00021806_003.png\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.0082706  -1.0494094  -1.274208   -1.0269717  -1.0026652  -1.2019366\n",
      " -0.9225463  -1.0989351  -1.064077   -1.196151   -0.92037153  0.5160062\n",
      " -0.8322891  -1.2229972  -1.1301031 ]\n",
      "00007780_002.png\n",
      "[1 1 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.80633104 -1.0821648  -0.76388204 -0.91912675 -0.77286994 -0.9096451\n",
      " -1.1928476  -1.0457113  -0.79848313 -0.9562408  -1.0774009  -0.3521576\n",
      " -1.1031619  -0.9703637  -0.7988548 ]\n",
      "00016424_003.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-0.74384266 -1.1902444  -0.8771511  -0.957037   -0.8564289  -0.95529014\n",
      " -1.0501412  -0.91405654 -0.7811128  -1.3045312  -1.0909866   0.193611\n",
      " -1.0105156  -1.1157488  -1.2068064 ]\n",
      "00011193_009.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-0.98043483 -0.92282295 -1.182822   -1.2017614  -1.1800795  -0.9638113\n",
      " -0.9050504  -1.02261    -0.8864766  -1.0099086  -1.2054117   0.4704721\n",
      " -0.8877472  -1.0806838  -1.089836  ]\n",
      "00013968_009.png\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.8689764  -1.1475798  -0.94679475 -1.1259952  -0.9801005  -0.92951345\n",
      " -1.1358682  -1.1007295  -1.1813828  -1.1594566  -1.2593387   0.6614832\n",
      " -1.1303097  -1.0854772  -1.0421863 ]\n",
      "00028918_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.1384599  -1.1437397  -0.98075104 -1.1409457  -0.85256827 -0.8787573\n",
      " -1.0713934  -1.1067698  -1.179256   -0.93804634 -0.9963182  -0.01526451\n",
      " -1.2504086  -1.0123641  -1.2897936 ]\n",
      "00001248_037.png\n",
      "[1 0 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
      "[-0.9216582  -1.118858   -0.9933175  -1.1397386  -0.9669256  -0.8801697\n",
      " -1.1636801  -1.0445155  -1.3101504  -1.0845342  -1.127719    0.44123763\n",
      " -1.3345109  -1.0337058  -1.2484635 ]\n",
      "00029579_001.png\n",
      "[0 0 0 0 1 0 0 0 1 0 0 0 0 0 1]\n",
      "[-1.1180456  -1.2558693  -1.0095906  -1.1156509  -1.1090182  -1.2002119\n",
      " -0.92088807 -0.86993    -0.7591889  -1.178391   -0.95821786  0.30191603\n",
      " -1.1215467  -1.0923727  -1.0412128 ]\n",
      "00006499_003.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.0338804  -1.2276374  -1.2008773  -0.999661   -1.1026648  -1.0310674\n",
      " -0.9494877  -1.0583481  -1.0087513  -1.0877827  -1.1244521   0.39721847\n",
      " -1.1117741  -0.99288976 -1.145656  ]\n",
      "00012625_014.png\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[-1.1963913  -1.1576567  -1.1557125  -1.0138164  -1.1722214  -1.1137733\n",
      " -0.9832938  -1.0848323  -0.94426686 -1.0382023  -1.0928589   0.07149749\n",
      " -1.0382903  -1.2111942  -0.9070719 ]\n",
      "00001787_013.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.1382304  -1.1671717  -1.0581958  -0.7792825  -0.7009477  -0.84660006\n",
      " -0.993896   -1.0505416  -1.0398636  -1.0813351  -0.9942943   0.03830203\n",
      " -1.137419   -0.92046666 -1.2685916 ]\n",
      "00024851_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.074324   -1.1312392  -0.93890977 -1.0576437  -1.3711641  -1.0068135\n",
      " -1.1813899  -1.2571092  -0.951185   -1.1128848  -0.86956966  0.26734596\n",
      " -0.92201    -0.81929827 -1.0839796 ]\n",
      "00030410_005.png\n",
      "[1 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      "[-0.81637263 -1.0778632  -1.0628407  -0.8838155  -1.2552891  -1.0224438\n",
      " -1.1884246  -1.1610892  -0.8766214  -1.1150687  -1.2280457   0.2879706\n",
      " -0.92545354 -1.0186731  -1.0882874 ]\n",
      "00006499_006.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "[-1.3059576  -0.890591   -1.0914904  -1.1287307  -1.0888444  -1.2474182\n",
      " -1.1408601  -1.1640435  -0.8989248  -1.1924813  -0.95906585  0.51182556\n",
      " -1.1123862  -1.0382886  -1.0875285 ]\n",
      "00017934_000.png\n",
      "[0 1 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "[-0.88384545 -1.0769323  -0.8032143  -0.96969223 -0.55922425 -0.8311164\n",
      " -1.0272185  -1.0195204  -0.72851396 -1.1252303  -1.1470042  -0.35138762\n",
      " -0.9352676  -0.9212891  -1.2050843 ]\n",
      "00030410_006.png\n",
      "[1 0 1 0 1 0 0 0 0 1 0 0 0 0 0]\n",
      "[-1.2842045  -1.2530934  -1.0179256  -0.946525   -0.95382065 -1.0258218\n",
      " -1.0301361  -1.0213417  -0.709707   -1.0682467  -1.1246316  -0.08147699\n",
      " -1.1103892  -1.2926834  -1.0521189 ]\n",
      "00014477_001.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.3853421  -1.0435073  -1.3980908  -1.0539217  -0.7920048  -1.1832553\n",
      " -1.0138651  -1.0059948  -0.804946   -1.3066659  -0.8723656   0.06678721\n",
      " -1.2209759  -1.2612656  -0.7343862 ]\n",
      "00011193_005.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.0045122  -0.8616034  -1.0623518  -1.2382276  -1.0419513  -1.0126362\n",
      " -1.1396405  -1.0409931  -0.74937606 -0.99374366 -1.300645    0.32283422\n",
      " -1.2526295  -1.1143085  -1.0791878 ]\n",
      "00028918_007.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.1840094  -0.92168105 -0.9813436  -0.94120216 -0.76753604 -0.9894702\n",
      " -1.2658173  -1.129853   -0.93717915 -1.0671123  -1.0162853  -0.02578732\n",
      " -1.2296017  -1.0553718  -0.92712617]\n",
      "00012681_033.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.94407254 -0.9718814  -0.75720227 -0.9896293  -0.2979375  -0.98489225\n",
      " -1.0239371  -1.101484   -0.6452191  -0.92929375 -0.908248   -0.94446915\n",
      " -1.0437868  -1.1070862  -0.81492174]\n",
      "00008142_004.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.2556479  -1.0650487  -0.8703979  -1.0669988  -0.91713387 -0.968095\n",
      " -1.0571345  -1.2194147  -0.90501916 -0.9502948  -1.0495374   0.38685855\n",
      " -0.9472297  -1.0137982  -0.98807794]\n",
      "00012643_000.png\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.90460694 -1.0579873  -0.61423326 -1.228013   -0.7087121  -0.847159\n",
      " -1.3864603  -1.146427   -0.76992136 -0.87440205 -1.2008559  -0.15041514\n",
      " -1.2852937  -1.1452717  -0.95973027]\n",
      "00029707_000.png\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.986209   -1.1846471  -0.84121627 -0.9605592  -0.7442201  -1.1091281\n",
      " -1.0122168  -0.90273666 -0.7702203  -1.1489557  -1.0180941  -0.24677263\n",
      " -1.1958559  -1.23631    -0.9575594 ]\n",
      "00001248_019.png\n",
      "[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.906746   -0.88500875 -1.378018   -1.0080035  -0.7516708  -1.0740616\n",
      " -1.0130721  -1.1656188  -1.0577717  -1.003458   -1.0879996   0.23929067\n",
      " -1.0138527  -0.9686456  -1.1539758 ]\n",
      "00029579_002.png\n",
      "[1 0 0 0 1 0 0 0 0 0 0 0 1 0 1]\n",
      "[-1.0337604  -1.1175793  -1.204738   -0.99155945 -1.1473669  -1.1149232\n",
      " -1.1315576  -1.0690364  -0.9049957  -1.1983199  -1.1408877   0.0830562\n",
      " -1.0925953  -1.2546618  -0.87738985]\n",
      "00004085_005.png\n",
      "[1 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
      "[-1.3390684 -1.0479397 -1.1410933 -1.3391916 -1.3452871 -1.0470247\n",
      " -1.0579196 -1.19489   -1.2159091 -0.9627815 -0.9733433  0.6941923\n",
      " -1.0379128 -0.9571468 -1.1220219]\n",
      "00002169_000.png\n",
      "[1 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.1036648  -1.0426701  -1.1914711  -0.8840553  -1.1184782  -1.1515388\n",
      " -1.0047941  -1.0818144  -0.95503604 -1.0814844  -0.90248704  0.2877729\n",
      " -1.057538   -0.8950897  -1.1171048 ]\n",
      "00001787_005.png\n",
      "[0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[-0.7364073  -0.99862957 -0.8869245  -0.9841294  -0.6806369  -0.99343276\n",
      " -0.91274416 -1.0725838  -0.6102365  -0.7236639  -1.075443   -0.3752299\n",
      " -0.9459703  -1.0840896  -0.82734996]\n",
      "00012409_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.0477488  -1.002854   -1.0412484  -1.121746   -0.9387202  -1.1521963\n",
      " -0.99811214 -1.025957   -0.63189876 -0.8773844  -0.99720514 -0.04976429\n",
      " -1.1414454  -1.0618552  -0.87562144]\n",
      "00001248_013.png\n",
      "[1 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
      "[-0.72096467 -1.0684097  -0.93522096 -0.8824783  -0.79540926 -0.9313703\n",
      " -1.0652431  -1.0137374  -0.6167813  -1.05404    -1.1869476  -0.27535826\n",
      " -0.93827766 -0.97140133 -1.0160875 ]\n",
      "00014738_000.png\n",
      "[1 0 0 0 1 0 0 0 1 1 0 0 0 0 0]\n",
      "[-1.1755294  -1.0808389  -1.1505324  -0.9141654  -0.9967012  -1.1338437\n",
      " -0.8120591  -0.98529124 -0.8275622  -1.0032407  -1.0383388  -0.09799315\n",
      " -0.9560978  -0.9862664  -0.899767  ]\n",
      "00026129_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-0.9949664  -1.0344634  -1.007397   -0.9918767  -0.83506835 -0.9136567\n",
      " -0.9931073  -1.2140226  -0.77551705 -0.9714725  -1.11268    -0.23037831\n",
      " -0.9196497  -1.0055138  -0.82835007]\n",
      "00022954_000.png\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "[-0.7522811  -0.9954097  -1.0520965  -0.97756624 -0.7095082  -1.0492646\n",
      " -0.95251215 -1.0095762  -0.4902507  -1.0401334  -1.0221593  -0.08023244\n",
      " -0.8189011  -1.1160779  -0.8579065 ]\n",
      "00000274_001.png\n",
      "[1 1 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.0346968  -1.0906278  -0.8667406  -0.988054   -0.9192339  -1.0325096\n",
      " -0.9441321  -1.0818784  -0.87452817 -0.95374095 -0.92231226 -0.32737744\n",
      " -0.86523837 -1.1048578  -1.0134844 ]\n",
      "00006113_002.png\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "[-0.9854318  -1.0183634  -1.1397295  -0.9170271  -0.9508811  -1.1551018\n",
      " -1.0701884  -1.05835    -0.7526072  -1.3098928  -0.83528614 -0.06687982\n",
      " -1.0539865  -1.0380188  -1.0769472 ]\n",
      "00008896_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.4264758 -1.1646649 -1.1021925 -1.1184801 -1.1820765 -1.0008719\n",
      " -0.9957189 -1.152706  -1.28619   -1.0559231 -1.1306167  0.5506923\n",
      " -1.240162  -0.9960873 -1.0583756]\n",
      "00019470_000.png\n",
      "[1 0 0 0 1 0 0 1 0 0 0 0 1 0 0]\n",
      "[-0.7328146  -1.1315882  -0.86553156 -1.031108   -1.0188067  -1.1513479\n",
      " -1.1789012  -1.0784147  -1.0332332  -1.0650377  -0.84856355  0.25044924\n",
      " -0.87645626 -1.1014409  -0.97308934]\n",
      "00030410_000.png\n",
      "[1 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "[-1.0681823  -1.1962934  -1.0028446  -0.9029772  -1.0540974  -1.0509622\n",
      " -0.8856844  -1.0783746  -0.62862414 -0.94627166 -0.99431556 -0.20913093\n",
      " -0.9922269  -0.9634501  -1.141202  ]\n",
      "00017934_001.png\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.581794   -1.2158746  -1.0873494  -1.1539862  -1.3441639  -1.0606515\n",
      " -1.0552831  -0.9793256  -0.8828607  -1.1945858  -1.0675199   0.72972286\n",
      " -1.2022196  -0.9971471  -1.1814667 ]\n",
      "00017689_000.png\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.3517474  -1.0479914  -1.3134656  -1.029708   -0.8794557  -1.0893526\n",
      " -0.80875134 -1.1426789  -0.9969087  -0.81523234 -0.9139373   0.39737645\n",
      " -1.1507074  -1.0202805  -1.0120289 ]\n",
      "00006893_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.543531  -1.0727248 -1.3018608 -1.1524122 -1.624992  -1.0965809\n",
      " -1.1343429 -1.1641126 -1.7762909 -1.182092  -1.0886638  1.1200343\n",
      " -1.0673826 -0.9678099 -1.366071 ]\n",
      "00008142_001.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.169108   -1.1008786  -1.059816   -0.88112605 -1.1914091  -1.1080577\n",
      " -1.050163   -1.0605216  -1.1598593  -0.98287463 -0.90246433 -0.01027429\n",
      " -1.0028213  -0.764907   -1.1884509 ]\n",
      "00001248_036.png\n",
      "[1 0 1 0 1 0 0 0 0 0 0 0 0 0 1]\n",
      "[-0.974893   -0.9036237  -0.754755   -1.1248386  -0.9751451  -0.8546069\n",
      " -1.1662385  -1.2001014  -1.0900716  -0.7370877  -1.2556634   0.16900171\n",
      " -1.0990927  -0.98069227 -0.89967805]\n",
      "00028918_005.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.8707519  -0.86774176 -0.8957418  -1.1405495  -0.219737   -0.9628681\n",
      " -0.935717   -1.1586206  -0.76886463 -0.6997215  -0.97585636 -0.3069644\n",
      " -1.1528236  -1.044092   -0.92225605]\n",
      "00013968_003.png\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "[-1.3636093  -1.1718471  -0.89901817 -1.1767912  -0.97001815 -0.9493555\n",
      " -1.1448474  -1.0950933  -0.9630699  -0.8285053  -1.0237188   0.32253224\n",
      " -1.3163947  -0.967234   -0.98716104]\n",
      "00012681_022.png\n",
      "[1 0 1 0 0 0 0 0 1 0 0 0 0 1 0]\n",
      "[-0.6999896  -1.205649   -0.92508435 -0.99461234 -0.88727605 -1.2102491\n",
      " -1.235149   -0.9430896  -0.7007817  -1.2020288  -0.9186149  -0.32884902\n",
      " -1.2015171  -0.9790109  -1.0718154 ]\n",
      "00012681_003.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "[-0.96741503 -1.0527056  -0.7726077  -0.97700053 -0.36662084 -0.8745472\n",
      " -1.0878456  -1.0551494  -0.6134951  -1.0292945  -1.068676   -0.47486383\n",
      " -1.2828051  -1.0020611  -0.85303223]\n",
      "00012681_025.png\n",
      "[0 0 1 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[-1.1071972  -1.0369575  -0.8456993  -1.1641324  -0.77969146 -0.9823198\n",
      " -1.0581429  -1.0621303  -0.53123707 -1.1569916  -0.9292482  -0.50508136\n",
      " -0.9810813  -1.2335391  -0.787995  ]\n",
      "00019906_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.9849475  -1.1238688  -1.0854801  -0.9689967  -0.77903557 -1.1012079\n",
      " -1.0876371  -0.962304   -0.71426857 -1.066166   -0.9900022   0.02679321\n",
      " -1.1284462  -1.2126958  -0.819814  ]\n",
      "00001248_034.png\n",
      "[1 0 1 0 1 0 0 0 0 0 0 0 0 0 1]\n",
      "[-1.105694   -1.1204375  -1.2305334  -0.8935053  -1.0025746  -1.2959678\n",
      " -0.96920687 -1.0215039  -0.8315353  -1.1411332  -1.0793228   0.30396628\n",
      " -1.0607824  -1.0509131  -1.1552451 ]\n",
      "00009647_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.2599438  -1.0290744  -1.1606575  -1.1384085  -1.327925   -1.051977\n",
      " -1.0315737  -0.9990399  -1.3449595  -1.1571599  -1.0262138   0.35104913\n",
      " -1.0501039  -1.0593287  -1.1615107 ]\n",
      "00030410_008.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-1.1786811  -1.0565912  -0.9126654  -1.0890982  -1.2579885  -1.0793494\n",
      " -1.2873118  -1.0739113  -0.88463265 -1.1223028  -1.1094787   0.21077096\n",
      " -1.1875578  -1.2317562  -0.9373737 ]\n",
      "00013968_010.png\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[-0.82995087 -1.1038295  -1.0892361  -1.0319586  -1.1532587  -1.0505725\n",
      " -0.9159905  -1.1679573  -1.3004391  -0.9847413  -1.0359353   0.02511635\n",
      " -0.80831134 -1.0301434  -1.0663434 ]\n",
      "00012681_045.png\n",
      "[1 0 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "[-0.8659482 -1.0567687 -1.154168  -1.0423974 -1.0239553 -1.0306975\n",
      " -1.1948336 -0.9719411 -0.8931944 -1.1455576 -0.9801071 -0.2721725\n",
      " -1.0791065 -1.1155559 -1.0777977]\n",
      "00013968_011.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.1191685  -1.1536939  -1.264018   -1.0814773  -1.2083465  -1.2945187\n",
      " -0.8568104  -1.035087   -0.92432916 -1.1757092  -0.7729335   0.38519132\n",
      " -0.81723565 -1.097022   -0.97339356]\n",
      "00001248_006.png\n",
      "[1 0 0 0 1 0 0 0 0 0 0 0 1 0 1]\n",
      "[-1.1963471  -1.2147077  -1.040243   -0.857347   -0.87417    -1.05834\n",
      " -0.9942151  -0.94452155 -0.65659547 -1.2089361  -1.1882682  -0.23745091\n",
      " -1.2051528  -1.2988224  -0.84181297]\n",
      "00022677_004.png\n",
      "[0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[-0.7187027  -1.1448861  -1.0070152  -1.0231732  -1.1386256  -1.010995\n",
      " -1.1263578  -0.944767   -0.8619191  -1.0421252  -1.2452335   0.05829202\n",
      " -1.034969   -1.0135458  -1.1476563 ]\n",
      "00001248_010.png\n",
      "[1 0 1 0 1 0 0 0 0 0 0 0 1 0 0]\n",
      "[-0.77613676 -1.078147   -0.81485105 -0.88635284 -0.8319613  -0.9970053\n",
      " -1.2218602  -1.0491586  -0.6560237  -0.94556266 -1.22296    -0.5371032\n",
      " -1.0063276  -0.9905817  -1.219517  ]\n",
      "00029579_009.png\n",
      "[1 0 0 0 0 0 0 0 0 1 1 0 1 0 0]\n",
      "[-0.88826656 -1.0458789  -0.97349304 -1.0009888  -0.8448758  -0.9637\n",
      " -1.0383265  -1.1053392  -0.4167872  -0.98567605 -1.2763473  -0.14627565\n",
      " -1.0807408  -1.030789   -0.9542892 ]\n",
      "00029748_000.png\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[-1.1781949  -1.1950686  -1.0210371  -0.9010208  -1.0611949  -1.1366352\n",
      " -1.2687331  -1.1864825  -0.9603277  -1.1085663  -1.058064    0.56404227\n",
      " -1.089418   -1.1045688  -1.0390503 ]\n"
     ]
    }
   ],
   "source": [
    "for doc in golden_dr_documents:\n",
    "    print(doc['filename'])\n",
    "    print(doc['labels'])\n",
    "    print(doc['pred_labels'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import scipy.misc as misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json(documents):\n",
    "    \n",
    "    for doc in documents:\n",
    "        json_item = {\n",
    "            #'id':doc['id_'],\n",
    "            'filename': doc['filename'],\n",
    "            'subset': SUBSET,\n",
    "            'labels': doc['labels'].tolist(),\n",
    "            #'age': int(doc['age']),\n",
    "            #'gender': doc['gender'],\n",
    "            #'view': doc['view'],\n",
    "            'mu':doc['mu'].tolist(),\n",
    "            'sigma':doc['sigma'].tolist()\n",
    "        }\n",
    "\n",
    "        if(SUBSET == 'golden_src'):\n",
    "            JSON_DIR = os.path.join(GOLDEN_EMBEDDINGS_DIR, doc['filename'][:12] + \"_src.json\")\n",
    "        elif(SUBSET == 'golden_dr'):\n",
    "            JSON_DIR = os.path.join(GOLDEN_EMBEDDINGS_DIR, doc['filename'][:12] + \"_dr.json\")\n",
    "        else:\n",
    "            JSON_DIR = os.path.join(GOLDEN_EMBEDDINGS_DIR, doc['filename'][:12] + \".json\")\n",
    "            \n",
    "        with open(JSON_DIR, 'w') as outfile:\n",
    "            json.dump(json_item, outfile, separators=(',', ':'), indent = 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBSET == \"golden_dr\":\n",
    "    gen_json(golden_dr_documents)\n",
    "    \n",
    "elif SUBSET == \"golden_src\":\n",
    "    gen_json(golden_src_documents)\n",
    "\n",
    "elif SUBSET == \"validate\":\n",
    "    gen_json(validate_documents)\n",
    "    \n",
    "elif SUBSET == \"test\":\n",
    "    gen_json(test_documents)\n",
    "    \n",
    "elif SUBSET == \"train\":\n",
    "    gen_json(train_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save the encoded 256x256 vectors as png file\n",
    "def save_images(documents):\n",
    "    for doc in documents:\n",
    "\n",
    "        path = os.path.join(ENCODED_IMAGES_DIR, doc['filename'])\n",
    "        misc.imsave(path, doc['xhat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "if SUBSET == \"golden\":\n",
    "    save_images(golden_documents)\n",
    "\n",
    "elif SUBSET == \"validate\":\n",
    "    save_images(validate_documents)\n",
    "    \n",
    "elif SUBSET == \"test\":\n",
    "    save_images(test_documents)\n",
    "    \n",
    "elif SUBSET == \"train\":\n",
    "    save_images(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14619\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(train_documents):\n",
    "    f = 0\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
